{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'llm_response': '•Oracle Instance maintenance, \\n•Good with standard DBA activities like installation, configuration and troubleshooting issues\\n•Hardening of databases and implementing database best practices\\n•Experience with Database refresh and clone activities\\n•Strong Knowledge of Database Backup and recovery using RMAN and logical backups.\\n•Ability to collaborate independently on critical issues with clients and provide root cause analysis\\n•Worked with ADDM/ASH/AWR reports and performance tuning.\\n•Good understanding of 3-tier application architecture\\n•Good communication skills\\n•Basic RedHat/Linux Administration\\n•Basic scripting skills with Unix Shell Scripting, PowerShell, Python\\n•Understanding of OEM and Oracle grid-control.\\n•Knowledge of Oracle EBS, SQL Server and administration is a plus\\n•Development background and knowledge of software development life cycle and tools (Jenkins, GitHub, Python)', 'prompt': 'Summarize the job description', 'evidence': 'Title: Database Admin Consultant II                              \\nReports to: Lead DBA / Manager\\nDirect Reports: None           \\nLocation: Bangalore, India \\n\\nDescription:\\nThe Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers.\\n\\nRequired skills:\\nOracle Instance maintenance, \\n\\n8-10 years of experience in Oracle Instance maintenance and troubleshooting role\\nGood with standard DBA activities like installation, configuration and troubleshooting issues\\nHardening of databases and implementing database best practices\\nExperience with Database refresh and clone activities\\nStrong Knowledge of Database Backup and recovery using RMAN and logical backups.\\nAbility to collaborate independently on critical issues with clients and provide root cause analysis\\nWorked with ADDM/ASH/AWR reports and performance tuning.\\nGood understanding of 3-tier application architecture\\nGood communication skills\\nDesired skills:\\nBasic RedHat/Linux Administration\\nBasic scripting skills with Unix Shell Scripting, PowerShell, Python\\nUnderstanding of OEM and Oracle grid-control.\\nKnowledge of Oracle EBS, SQL Server and administration is a plus\\nDevelopment background and knowledge of software development life cycle and tools (Jenkins, GitHub, Python)\\n', 'instruction': 'default_with_context', 'model': 'llmware/bling-tiny-llama-v0', 'usage': {'input': 314, 'output': 200, 'total': 514, 'metric': 'tokens', 'processing_time': 82.12586450576782}, 'time_stamp': '2024-09-01_150243', 'calling_app_ID': '', 'rating': '', 'account_name': 'llmware', 'prompt_id': 0, 'batch_id': 0, 'evidence_metadata': [{'evidence_start_char': 0, 'evidence_stop_char': 1428, 'page_num': 'NA', 'source_name': 'NA', 'doc_id': 'NA', 'block_id': 'NA'}]}\n",
      "Time taken: 82.13 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Path to your .docx file\n",
    "file_path = r'C:\\DataScience-TechERG\\LLM_JD\\Oracle DBA_V1.docx'\n",
    "\n",
    "# Read the document content\n",
    "doc_content = read_docx(file_path)\n",
    "\n",
    "# Define the prompt and context\n",
    "prompt = \"Summarize the job description\"\n",
    "context = doc_content  # Using the content read from the .docx file as the context\n",
    "\n",
    "# Define model name before using it\n",
    "model_name = \"llmware/bling-tiny-llama-v0\"\n",
    "\n",
    "# Initialize the prompter with the specified model\n",
    "prompter = Prompt().load_model(model_name)\n",
    "\n",
    "# Record start time for operation\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate a response using the prompter\n",
    "response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "# Calculate and print the time taken for the operation\n",
    "time_taken = round(time.time() - start_time, 2)\n",
    "\n",
    "# Print the response and time taken\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Time taken: {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llmware/bling-falcon-1b-0.1\n",
      "Response: {'llm_response': ' The Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers.                                                                                                                                                                ', 'prompt': 'Summarize the job description', 'evidence': 'Title: Database Admin Consultant II                              \\nReports to: Lead DBA / Manager\\nDirect Reports: None           \\nLocation: Bangalore, India \\n\\nDescription:\\nThe Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers.\\n\\nRequired skills:\\nOracle Instance maintenance, \\n\\n8-10 years of experience in Oracle Instance maintenance and troubleshooting role\\nGood with standard DBA activities like installation, configuration and troubleshooting issues\\nHardening of databases and implementing database best practices\\nExperience with Database refresh and clone activities\\nStrong Knowledge of Database Backup and recovery using RMAN and logical backups.\\nAbility to collaborate independently on critical issues with clients and provide root cause analysis\\nWorked with ADDM/ASH/AWR reports and performance tuning.\\nGood understanding of 3-tier application architecture\\nGood communication skills\\nDesired skills:\\nBasic RedHat/Linux Administration\\nBasic scripting skills with Unix Shell Scripting, PowerShell, Python\\nUnderstanding of OEM and Oracle grid-control.\\nKnowledge of Oracle EBS, SQL Server and administration is a plus\\nDevelopment background and knowledge of software development life cycle and tools (Jenkins, GitHub, Python)\\n', 'instruction': 'default_with_context', 'model': 'llmware/bling-falcon-1b-0.1', 'usage': {'input': 320, 'output': 200, 'total': 520, 'metric': 'tokens', 'processing_time': 142.6464433670044}, 'time_stamp': '2024-07-05_115318', 'calling_app_ID': '', 'rating': '', 'account_name': 'llmware', 'prompt_id': 0, 'batch_id': 0, 'evidence_metadata': [{'evidence_start_char': 0, 'evidence_stop_char': 1428, 'page_num': 'NA', 'source_name': 'NA', 'doc_id': 'NA', 'block_id': 'NA'}]}\n",
      "Time taken: 142.69 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llmware/bling-falcon-1b-0.1\n",
      "Response: {'llm_response': ' The Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers.                                                                                                                                                                ', 'prompt': 'Summarize the job description', 'evidence': 'Title: Database Admin Consultant II                              \\nReports to: Lead DBA / Manager\\nDirect Reports: None           \\nLocation: Bangalore, India \\n\\nDescription:\\nThe Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers.\\n\\nRequired skills:\\nOracle Instance maintenance, \\n\\n8-10 years of experience in Oracle Instance maintenance and troubleshooting role\\nGood with standard DBA activities like installation, configuration and troubleshooting issues\\nHardening of databases and implementing database best practices\\nExperience with Database refresh and clone activities\\nStrong Knowledge of Database Backup and recovery using RMAN and logical backups.\\nAbility to collaborate independently on critical issues with clients and provide root cause analysis\\nWorked with ADDM/ASH/AWR reports and performance tuning.\\nGood understanding of 3-tier application architecture\\nGood communication skills\\nDesired skills:\\nBasic RedHat/Linux Administration\\nBasic scripting skills with Unix Shell Scripting, PowerShell, Python\\nUnderstanding of OEM and Oracle grid-control.\\nKnowledge of Oracle EBS, SQL Server and administration is a plus\\nDevelopment background and knowledge of software development life cycle and tools (Jenkins, GitHub, Python)\\n', 'instruction': 'default_with_context', 'model': 'llmware/bling-falcon-1b-0.1', 'usage': {'input': 320, 'output': 200, 'total': 520, 'metric': 'tokens', 'processing_time': 118.80892133712769}, 'time_stamp': '2024-07-05_115939', 'calling_app_ID': '', 'rating': '', 'account_name': 'llmware', 'prompt_id': 0, 'batch_id': 0, 'evidence_metadata': [{'evidence_start_char': 0, 'evidence_stop_char': 1428, 'page_num': 'NA', 'source_name': 'NA', 'doc_id': 'NA', 'block_id': 'NA'}]}\n",
      "Time taken: 118.85 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llmware/bling-falcon-1b-0.1\n",
      "Response: {'llm_response': ' The Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers.                                                                                                                                                                ', 'prompt': 'Summarize the job description', 'evidence': 'Title: Database Admin Consultant II                              \\nReports to: Lead DBA / Manager\\nDirect Reports: None           \\nLocation: Bangalore, India \\n\\nDescription:\\nThe Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers.\\n\\nRequired skills:\\nOracle Instance maintenance, \\n\\n8-10 years of experience in Oracle Instance maintenance and troubleshooting role\\nGood with standard DBA activities like installation, configuration and troubleshooting issues\\nHardening of databases and implementing database best practices\\nExperience with Database refresh and clone activities\\nStrong Knowledge of Database Backup and recovery using RMAN and logical backups.\\nAbility to collaborate independently on critical issues with clients and provide root cause analysis\\nWorked with ADDM/ASH/AWR reports and performance tuning.\\nGood understanding of 3-tier application architecture\\nGood communication skills\\nDesired skills:\\nBasic RedHat/Linux Administration\\nBasic scripting skills with Unix Shell Scripting, PowerShell, Python\\nUnderstanding of OEM and Oracle grid-control.\\nKnowledge of Oracle EBS, SQL Server and administration is a plus\\nDevelopment background and knowledge of software development life cycle and tools (Jenkins, GitHub, Python)\\n', 'instruction': 'default_with_context', 'model': 'llmware/bling-falcon-1b-0.1', 'usage': {'input': 320, 'output': 200, 'total': 520, 'metric': 'tokens', 'processing_time': 111.06563210487366}, 'time_stamp': '2024-07-05_120618', 'calling_app_ID': '', 'rating': '', 'account_name': 'llmware', 'prompt_id': 0, 'batch_id': 0, 'evidence_metadata': [{'evidence_start_char': 0, 'evidence_stop_char': 1428, 'page_num': 'NA', 'source_name': 'NA', 'doc_id': 'NA', 'block_id': 'NA'}]}\n",
      "Time taken: 111.08 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.models import ModelCatalog\n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Path to your .docx file\n",
    "file_path = r'C:\\DataScience-TechERG\\LLM_JD\\Oracle DBA_V1.docx'\n",
    "\n",
    "# Read the document content\n",
    "doc_content = read_docx(file_path)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Summarize the job description\"\n",
    "context = doc_content  # Using the content read from the .docx file as the context\n",
    "\n",
    "# List of models to use\n",
    "models = [\"llmware/bling-1b-0.1\", \"llmware/bling-tiny-llama-v0\", \"llmware/bling-falcon-1b-0.1\"]\n",
    "\n",
    "# Initialize the prompter\n",
    "prompter = Prompt()\n",
    "\n",
    "for model_name in models:\n",
    "    # Load the model from the catalog\n",
    "    prompter.load_model(model_name)\n",
    "\n",
    "    # Record start time for operation\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate a response using the prompter\n",
    "    response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "    # Calculate and print the time taken for the operation\n",
    "    time_taken = round(time.time() - start_time, 2)\n",
    "\n",
    "    # Print the model, response, and time taken\n",
    "    #print(f\"Model: {model_name}\")\n",
    "    #description = response['llm_response'].get('Description', 'Description not found')\n",
    "    #print(f\"Description: {description}\")\n",
    "    #print(f\"Response: {response}\")\n",
    "    #print(f\"Time taken: {time_taken} seconds\\n\")\n",
    "\n",
    "    for model_name in models:\n",
    "    # Load the model from the catalog\n",
    "       prompter.load_model(model_name)\n",
    "\n",
    "    # Record start time for operation\n",
    "       start_time = time.time()\n",
    "\n",
    "    # Generate a response using the prompter\n",
    "       response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "    # Calculate and print the time taken for the operation\n",
    "       time_taken = round(time.time() - start_time, 2)\n",
    "\n",
    "    # Print the model, response, and time taken\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Response: {response}\")  # Directly print the response\n",
    "    print(f\"Time taken: {time_taken} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_name': 'all-MiniLM-L6-v2',\n",
       "  'display_name': 'mini-lm-sbert',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 384,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'sentence-transformers/all-MiniLM-L6-v2'},\n",
       " {'model_name': 'all-mpnet-base-v2',\n",
       "  'display_name': 'mpnet-base',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 514,\n",
       "  'link': 'https://huggingface.co/sentence-transformers/all-mpnet-base-v2',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'sentence-transformers/all-mpnet-base-v2'},\n",
       " {'model_name': 'industry-bert-insurance',\n",
       "  'display_name': 'industry-bert-insurance',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/llmware/industry-bert-insurance-v0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/industry-bert-insurance-v0.1'},\n",
       " {'model_name': 'industry-bert-contracts',\n",
       "  'display_name': 'industry-bert-contracts',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/llmware/industry-bert-contracts-v0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/industry-bert-contracts-v0.1'},\n",
       " {'model_name': 'industry-bert-asset-management',\n",
       "  'display_name': 'industry-bert-asset-management',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/llmware/industry-bert-asset-management-v0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/industry-bert-asset-management-v0.1'},\n",
       " {'model_name': 'industry-bert-sec',\n",
       "  'display_name': 'industry-bert-sec',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/llmware/industry-bert-sec-v0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/industry-bert-sec-v0.1'},\n",
       " {'model_name': 'industry-bert-loans',\n",
       "  'display_name': 'industry-bert-loans',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/llmware/industry-bert-loans',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/industry-bert-loans'},\n",
       " {'model_name': 'nomic-ai/nomic-embed-text-v1',\n",
       "  'display_name': 'nomic-text-v1',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 8192,\n",
       "  'link': 'https://huggingface.co/nomic-ai/nomic-embed-text-v1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'nomic-ai/nomic-embed-text-v1'},\n",
       " {'model_name': 'jinaai/jina-embeddings-v2-base-en',\n",
       "  'display_name': 'jina-base-en-v2',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 8192,\n",
       "  'link': 'https://huggingface.co/jinaai/jina-embeddings-v2-base-en',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'jinaai/jina-embeddings-v2-base-en'},\n",
       " {'model_name': 'jinaai/jina-embeddings-v2-small-en',\n",
       "  'display_name': 'jina-small-en-v2',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 512,\n",
       "  'context_window': 8192,\n",
       "  'link': 'https://huggingface.co/jinaai/jina-embeddings-v2-small-en',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'jinaai/jina-embeddings-v2-small-en'},\n",
       " {'model_name': 'BAAI/bge-small-en-v1.5',\n",
       "  'display_name': 'bge-small-en-v1.5',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 384,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/BAAI/bge-small-en-v1.5',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'BAAI/bge-small-en-v1.5'},\n",
       " {'model_name': 'BAAI/bge-large-en-v1.5',\n",
       "  'display_name': 'bge-large-en-v1.5',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 1024,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/BAAI/bge-large-en-v1.5',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'BAAI/bge-large-en-v1.5'},\n",
       " {'model_name': 'BAAI/bge-base-en-v1.5',\n",
       "  'display_name': 'bge-base-en-v1.5',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/BAAI/bge-base-en-v1.5',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'BAAI/bge-base-en-v1.5'},\n",
       " {'model_name': 'thenlper/gte-small',\n",
       "  'display_name': 'gte-small',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 384,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/thenlper/gte-small',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'thenlper/gte-small'},\n",
       " {'model_name': 'thenlper/gte-base',\n",
       "  'display_name': 'gte-base',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 768,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/thenlper/gte-base',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'thenlper/gte-base'},\n",
       " {'model_name': 'thenlper/gte-large',\n",
       "  'display_name': 'gte-large',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 1024,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/thenlper/gte-large',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'thenlper/gte-large'},\n",
       " {'model_name': 'llmrails/ember-v1',\n",
       "  'display_name': 'ember-v1',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 1024,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/llmrails/ember-v1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmrails/ember-v1'},\n",
       " {'model_name': 'WhereIsAI/UAE-Large-V1',\n",
       "  'display_name': 'uae-large-v1',\n",
       "  'model_family': 'HFEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 1024,\n",
       "  'context_window': 512,\n",
       "  'link': 'https://huggingface.co/WhereIsAI/UAE-Large-V1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'WhereIsAI/UAE-Large-V1'},\n",
       " {'model_name': 'text-embedding-ada-002',\n",
       "  'display_name': 'OpenAI-Embedding',\n",
       "  'model_family': 'OpenAIEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8191,\n",
       "  'embedding_dims': 1536},\n",
       " {'model_name': 'text-embedding-3-small',\n",
       "  'display_name': 'OpenAI-Embedding',\n",
       "  'model_family': 'OpenAIEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8191,\n",
       "  'embedding_dims': 1536},\n",
       " {'model_name': 'text-embedding-3-large',\n",
       "  'display_name': 'OpenAI-Embedding',\n",
       "  'model_family': 'OpenAIEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8191,\n",
       "  'embedding_dims': 3072},\n",
       " {'model_name': 'medium',\n",
       "  'display_name': 'Cohere-Medium-Embedding',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 4096},\n",
       " {'model_name': 'xlarge',\n",
       "  'display_name': 'Cohere-XLarge-Embedding',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 4096},\n",
       " {'model_name': 'embed-english-v3.0',\n",
       "  'display_name': 'Cohere-English-v3',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 1024},\n",
       " {'model_name': 'embed-multilingual-v3.0',\n",
       "  'display_name': 'Cohere-Multi-Lingual-v3',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 1024},\n",
       " {'model_name': 'embed-english-light-v3.0',\n",
       "  'display_name': 'Cohere-English-v3',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 384},\n",
       " {'model_name': 'embed-multilingual-light-v3.0',\n",
       "  'display_name': 'Cohere-English-v3',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 384},\n",
       " {'model_name': 'embed-english-v2.0',\n",
       "  'display_name': 'Cohere-English-v3',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 4096},\n",
       " {'model_name': 'embed-english-light-v2.0',\n",
       "  'display_name': 'Cohere-English-v3',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 1024},\n",
       " {'model_name': 'embed-multilingual-v2.0',\n",
       "  'display_name': 'Cohere-English-v3',\n",
       "  'model_family': 'CohereEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048,\n",
       "  'embedding_dims': 768},\n",
       " {'model_name': 'textembedding-gecko@latest',\n",
       "  'display_name': 'Google-Embedding',\n",
       "  'model_family': 'GoogleEmbeddingModel',\n",
       "  'model_category': 'embedding',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 4000,\n",
       "  'embedding_dims': 768},\n",
       " {'model_name': 'claude-v1',\n",
       "  'display_name': 'Anthropic Claude-v1',\n",
       "  'model_family': 'ClaudeModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8000},\n",
       " {'model_name': 'claude-instant-v1',\n",
       "  'display_name': 'claude-instant-1.2',\n",
       "  'model_family': 'ClaudeModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8000},\n",
       " {'model_name': 'claude-3-opus-20240229',\n",
       "  'display_name': 'Anthropic-Claude-3-Opus',\n",
       "  'model_family': 'ClaudeModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8192},\n",
       " {'model_name': 'claude-3-sonnet-20240229',\n",
       "  'display_name': 'Anthropic-Claude-3-Sonnet',\n",
       "  'model_family': 'ClaudeModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8192},\n",
       " {'model_name': 'claude-2.1',\n",
       "  'display_name': 'Anthropic Claude-2.1',\n",
       "  'model_family': 'ClaudeModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8192},\n",
       " {'model_name': 'claude-2.0',\n",
       "  'display_name': 'Anthropic Claude-Claude2-.0',\n",
       "  'model_family': 'ClaudeModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8192},\n",
       " {'model_name': 'command-medium-nightly',\n",
       "  'display_name': 'Cohere Command Medium',\n",
       "  'model_family': 'CohereGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'command-xlarge-nightly',\n",
       "  'display_name': 'Cohere Command XLarge',\n",
       "  'model_family': 'CohereGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'summarize-xlarge',\n",
       "  'display_name': 'Cohere Summarize Xlarge',\n",
       "  'model_family': 'CohereGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'summarize-medium',\n",
       "  'display_name': 'Cohere Summarize Medium',\n",
       "  'model_family': 'CohereGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'j2-jumbo-instruct',\n",
       "  'display_name': 'Jurassic-2-Jumbo-Instruct',\n",
       "  'model_family': 'JurassicModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'j2-grande-instruct',\n",
       "  'display_name': 'Jurassic-2-Grande-Instruct',\n",
       "  'model_family': 'JurassicModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'text-bison@001',\n",
       "  'display_name': 'Google Palm',\n",
       "  'model_family': 'GoogleGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8192},\n",
       " {'model_name': 'chat-bison@001',\n",
       "  'display_name': 'Google Chat',\n",
       "  'model_family': 'GoogleGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8192},\n",
       " {'model_name': 'text-davinci-003',\n",
       "  'display_name': 'GPT3-Davinci',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 4096},\n",
       " {'model_name': 'text-curie-001',\n",
       "  'display_name': 'GPT3-Curie',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'text-babbage-001',\n",
       "  'display_name': 'GPT3-Babbage',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'text-ada-001',\n",
       "  'display_name': 'GPT3-Ada',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'gpt-3.5-turbo',\n",
       "  'display_name': 'ChatGPT',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 4000},\n",
       " {'model_name': 'gpt-4',\n",
       "  'display_name': 'GPT-4',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 8000},\n",
       " {'model_name': 'gpt-3.5-turbo-instruct',\n",
       "  'display_name': 'GPT-3.5-Instruct',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 4000},\n",
       " {'model_name': 'gpt-4-1106-preview',\n",
       "  'display_name': 'GPT-4-Turbo-1106',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 128000},\n",
       " {'model_name': 'gpt-3.5-turbo-1106',\n",
       "  'display_name': 'GPT-3.5-Turbo-1106',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 16385},\n",
       " {'model_name': 'gpt-4-0125-preview',\n",
       "  'display_name': 'GPT-4-Turbo-0125',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 128000},\n",
       " {'model_name': 'gpt-3.5-turbo-0125',\n",
       "  'display_name': 'GPT-3.5-Turbo-0125',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 16385},\n",
       " {'model_name': 'gpt-4o',\n",
       "  'display_name': 'GPT-4o',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 128000},\n",
       " {'model_name': 'gpt-4o-2024-05-13',\n",
       "  'display_name': 'gpt-4o-2024-05-13',\n",
       "  'model_family': 'OpenAIGenModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 128000},\n",
       " {'model_name': 'llmware-inference-server',\n",
       "  'display_name': 'LLMWare-GPT',\n",
       "  'model_family': 'LLMWareModel',\n",
       "  'model_category': 'generative-api',\n",
       "  'model_location': 'api',\n",
       "  'context_window': 2048},\n",
       " {'model_name': 'llmware/bling-1.4b-0.1',\n",
       "  'display_name': 'bling-1.4b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-1.4b-0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-1.4b-0.1'},\n",
       " {'model_name': 'llmware/bling-1b-0.1',\n",
       "  'display_name': 'bling-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-1b-0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-1b-0.1'},\n",
       " {'model_name': 'llmware/bling-falcon-1b-0.1',\n",
       "  'display_name': 'bling-falcon-1.3b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-falcon-1b-0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-falcon-1b-0.1'},\n",
       " {'model_name': 'llmware/bling-sheared-llama-1.3b-0.1',\n",
       "  'display_name': 'bling-sheared-llama-1.3b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-sheared-llama-1.3b-0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-sheared-llama-1.3b-0.1'},\n",
       " {'model_name': 'llmware/bling-red-pajamas-3b-0.1',\n",
       "  'display_name': 'bling-red-pajamas-3b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-red-pajamas-3b-0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-red-pajamas-3b-0.1'},\n",
       " {'model_name': 'llmware/bling-sheared-llama-2.7b-0.1',\n",
       "  'display_name': 'bling-sheared-llama-2.7b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-sheared-llama-2.7b-0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-sheared-llama-2.7b-0.1'},\n",
       " {'model_name': 'llmware/bling-stable-lm-3b-4e1t-v0',\n",
       "  'display_name': 'bling-stablelm-3b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-stable-lm-3b-4e1t-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-stable-lm-3b-4e1t-v0'},\n",
       " {'model_name': 'llmware/bling-cerebras-1.3b-0.1',\n",
       "  'display_name': 'bling-cerebras-1.3b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-cerebras-1.3b-0.1',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-cerebras-1.3b-0.1'},\n",
       " {'model_name': 'llmware/bling-tiny-llama-v0',\n",
       "  'display_name': 'bling-tiny-llama-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-tiny-llama-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-tiny-llama-v0',\n",
       "  'standard': True},\n",
       " {'model_name': 'llmware/dragon-yi-6b-v0',\n",
       "  'display_name': 'dragon-yi-6b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '\\n',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-yi-6b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/dragon-yi-6b-v0'},\n",
       " {'model_name': 'llmware/dragon-stablelm-7b-v0',\n",
       "  'display_name': 'dragon-stablelm-7b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-stablelm-7b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/dragon-stablelm-7b-v0'},\n",
       " {'model_name': 'llmware/dragon-mistral-7b-v0',\n",
       "  'display_name': 'dragon-mistral-7b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-mistral-7b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/dragon-mistral-7b-v0'},\n",
       " {'model_name': 'llmware/dragon-red-pajama-7b-v0',\n",
       "  'display_name': 'dragon-red-pajama-7b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-red-pajama-7b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/dragon-red-pajama-7b-v0'},\n",
       " {'model_name': 'llmware/dragon-deci-6b-v0',\n",
       "  'display_name': 'dragon-deci-6b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-deci-6b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/dragon-deci-6b-v0'},\n",
       " {'model_name': 'llmware/dragon-falcon-7b-v0',\n",
       "  'display_name': 'dragon-falcon-7b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-falcon-7b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/dragon-falcon-7b-v0'},\n",
       " {'model_name': 'llmware/dragon-llama-7b-v0',\n",
       "  'display_name': 'dragon-llama-7b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-llama-7b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/dragon-llama-7b-v0'},\n",
       " {'model_name': 'llmware/dragon-deci-7b-v0',\n",
       "  'display_name': 'dragon-deci-7b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-deci-7b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/dragon-deci-7b-v0'},\n",
       " {'model_name': 'llmware/bling-phi-3',\n",
       "  'display_name': 'bling-phi-3',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/bling-phi-3',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/bling-phi-3'},\n",
       " {'model_name': 'bling-phi-3-gguf',\n",
       "  'display_name': 'llmware/bling-phi-3-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'bling-phi-3.gguf',\n",
       "  'gguf_repo': 'llmware/bling-phi-3-gguf',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['bling-phi-3.gguf'],\n",
       "  'tokenizer_local': 'tokenizer_phi3.json',\n",
       "  'link': 'https://huggingface.co/llmware/bling-phi-3-gguf',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'llmware/dragon-mistral-7b-gguf',\n",
       "  'display_name': 'dragon-mistral-7b-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['dragon-mistral-7b-q4_k_m.gguf'],\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'dragon-mistral-7b-q4_k_m.gguf',\n",
       "  'gguf_repo': 'llmware/dragon-mistral-7b-v0',\n",
       "  'link': 'https://huggingface.co/llmware/dragon-mistral-7b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'llmware/dragon-llama-7b-gguf',\n",
       "  'display_name': 'dragon-llama-7b-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'dragon-llama-7b-q4_k_m.gguf',\n",
       "  'gguf_repo': 'llmware/dragon-llama-7b-v0',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['dragon-llama-7b-q4_k_m.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/dragon-llama-7b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'llmware/dragon-yi-6b-gguf',\n",
       "  'display_name': 'dragon-yi-6b-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '\\n',\n",
       "  'gguf_file': 'dragon-yi-6b-q4_k_m.gguf',\n",
       "  'gguf_repo': 'llmware/dragon-yi-6b-v0',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['dragon-yi-6b-q4_k_m.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/dragon-yi-6b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'dragon-yi-answer-tool',\n",
       "  'display_name': 'dragon-yi-6b-answer-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '\\n',\n",
       "  'gguf_file': 'dragon-yi.gguf',\n",
       "  'gguf_repo': 'llmware/dragon-yi-answer-tool',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['dragon-yi.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/dragon-yi-answer-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'dragon-llama-answer-tool',\n",
       "  'display_name': 'dragon-llama-answer-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'dragon-llama.gguf',\n",
       "  'gguf_repo': 'llmware/dragon-llama-answer-tool',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['dragon-llama.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/dragon-llama-answer-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'dragon-mistral-answer-tool',\n",
       "  'display_name': 'dragon-mistral-answer-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'dragon-mistral.gguf',\n",
       "  'gguf_repo': 'llmware/dragon-mistral-answer-tool',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['dragon-mistral.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/dragon-mistral-answer-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'TheBloke/Llama-2-7B-Chat-GGUF',\n",
       "  'display_name': 'llama-2-7b-chat-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': True,\n",
       "  'prompt_wrapper': '<INST>',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'llama-2-7b-chat.Q4_K_M.gguf',\n",
       "  'gguf_repo': 'llmware/bonchon',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['llama-2-7b-chat.Q4_K_M.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/bonchon',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'TheBloke/OpenHermes-2.5-Mistral-7B-GGUF',\n",
       "  'display_name': 'openhermes-mistral-7b-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': True,\n",
       "  'prompt_wrapper': 'chat_ml',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'openhermes-2.5-mistral-7b.Q4_K_M.gguf',\n",
       "  'gguf_repo': 'llmware/bonchon',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['openhermes-2.5-mistral-7b.Q4_K_M.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/bonchon',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'TheBloke/zephyr-7B-beta-GGUF',\n",
       "  'display_name': 'zephyr-7b-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': True,\n",
       "  'prompt_wrapper': 'hf_chat',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'zephyr-7b-beta.Q4_K_M.gguf',\n",
       "  'gguf_repo': 'llmware/bonchon',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['zephyr-7b-beta.Q4_K_M.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/bonchon',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'TheBloke/Starling-LM-7B-alpha-GGUF',\n",
       "  'display_name': 'starling-7b-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': True,\n",
       "  'prompt_wrapper': 'open_chat',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'starling-lm-7b-alpha.Q4_K_M.gguf',\n",
       "  'gguf_repo': 'llmware/bonchon',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['starling-lm-7b-alpha.Q4_K_M.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/bonchon',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'microsoft/Phi-3-mini-4k-instruct-gguf',\n",
       "  'display_name': 'phi-3-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_windows': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'phi_3',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'Phi-3-mini-4k-instruct-q4.gguf',\n",
       "  'gguf_repo': 'microsoft/Phi-3-mini-4k-instruct-gguf',\n",
       "  'link': 'https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf',\n",
       "  'tokenizer_local': 'tokenizer_phi3.json',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['Phi-3-mini-4k-instruct-q4.gguf'],\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'microsoft/Phi-3-mini-4k-instruct',\n",
       "  'display_name': 'phi-3',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'phi_3',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'microsoft/Phi-3-mini-4k-instruct'},\n",
       " {'model_name': 'microsoft/Phi-3-mini-128k-instruct',\n",
       "  'display_name': 'phi-3-128k',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'phi_3',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-gguf',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'microsoft/Phi-3-mini-128k-instruct'},\n",
       " {'model_name': 'Meta-Llama-3-8B-Instruct',\n",
       "  'display_name': 'llama-3-instruct',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 8192,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'llama_3_chat',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/meta-llama/Meta-LLama-3-8B-instruct',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'meta-llama/Meta-Llama-3-8B-Instruct'},\n",
       " {'model_name': 'Meta-Llama-3-8B',\n",
       "  'display_name': 'llama-3-base',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 8192,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'llama_3_chat',\n",
       "  'temperature': 0.3,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/meta-llama/Meta-LLama-3-8B',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'meta-llama/Meta-Llama-3-8B'},\n",
       " {'model_name': 'QuantFactory/Meta-Llama-3-8B-Instruct-GGUF',\n",
       "  'display_name': 'llama-3-instruct-qf-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 8192,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'llama_3_chat',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'Meta-Llama-3-8B-Instruct.Q4_K_M.gguf',\n",
       "  'gguf_repo': 'QuantFactory/Meta-Llama-3-8B-Instruct-GGUF',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['Meta-Llama-3-8B-Instruct.Q4_K_M.gguf'],\n",
       "  'link': 'https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'QuantFactory/Meta-Llama-3-8B-GGUF',\n",
       "  'display_name': 'llama-3-base-qf-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 8192,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'llama_3_chat',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'Meta-Llama-3-8B.Q4_K_M.gguf',\n",
       "  'gguf_repo': 'QuantFactory/Meta-Llama-3-8B-GGUF',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['Meta-Llama-3-8B.Q4_K_M.gguf'],\n",
       "  'link': 'https://huggingface.co/QuantFactory/Meta-Llama-3-GGUF',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'bartowski/Meta-Llama-3-8B-Instruct-GGUF',\n",
       "  'display_name': 'llama-3-instruct-bartowski-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 8192,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'llama_3_chat',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'Meta-Llama-3-8B-Instruct-Q4_K_M.gguf',\n",
       "  'gguf_repo': 'bartowski/Meta-Llama-3-8B-Instruct-GGUF',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['Meta-Llama-3-8B-Instruct-Q4_K_M.gguf'],\n",
       "  'link': 'https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'tiny-llama-chat-gguf',\n",
       "  'display_name': 'tiny-llama-chat-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'hf_chat',\n",
       "  'temperature': 0.3,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'tiny-llama-chat.gguf',\n",
       "  'gguf_repo': 'llmware/bonchon',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['tiny-llama-chat.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/bonchon',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'whisper-cpp-base-english',\n",
       "  'display_name': 'whisper-en-base',\n",
       "  'model_family': 'WhisperCPPModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': '',\n",
       "  'temperature': 0.0,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'ggml-base.en.bin',\n",
       "  'gguf_repo': 'llmware/bonchon',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['ggml-base.en.bin'],\n",
       "  'link': 'https://huggingface.co/llmware/bonchon',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'whisper-cpp-base',\n",
       "  'display_name': 'whisper-base',\n",
       "  'model_family': 'WhisperCPPModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': '',\n",
       "  'temperature': 0.0,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'ggml-base.bin',\n",
       "  'gguf_repo': 'llmware/bonchon',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['ggml-base.bin'],\n",
       "  'link': 'https://huggingface.co/llmware/bonchon',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'whisper-cpp-tiny-diarize',\n",
       "  'display_name': 'whisper-en-tiny-diarize',\n",
       "  'model_family': 'WhisperCPPModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': '',\n",
       "  'temperature': 0.0,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'ggml-small.en-tdrz.bin',\n",
       "  'gguf_repo': 'llmware/bonchon',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_model_from_hf'},\n",
       "  'validation_files': ['ggml-small.en-trdz.bin'],\n",
       "  'link': 'https://huggingface.co/llmware/bonchon',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'slim-ner-tool',\n",
       "  'display_name': 'slim-ner-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-ner.gguf',\n",
       "  'gguf_repo': 'llmware/slim-ner-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-ner-tool',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-ner.gguf'],\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['people', 'location', 'organization', 'misc'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'standard': True},\n",
       " {'model_name': 'slim-sentiment-tool',\n",
       "  'display_name': 'slim-sentiment-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-sentiment.gguf',\n",
       "  'gguf_repo': 'llmware/slim-sentiment-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-sentiment-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['sentiment'],\n",
       "  'fc_output_values': ['positive', 'neutral', 'negative'],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [1066, 22198, 17821],\n",
       "  'marker_token_lookup': {1066: 'positive',\n",
       "   22198: 'negative',\n",
       "   17821: 'neutral'},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-sentiment.gguf']},\n",
       " {'model_name': 'slim-emotions-tool',\n",
       "  'display_name': 'slim-emotions-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-emotions.gguf',\n",
       "  'gguf_repo': 'llmware/slim-emotions-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-emotions-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['emotions'],\n",
       "  'fc_output_values': ['afraid',\n",
       "   'anger',\n",
       "   'angry',\n",
       "   'annoyed',\n",
       "   'anticipating',\n",
       "   'anxious',\n",
       "   'apprehensive',\n",
       "   'ashamed',\n",
       "   'caring',\n",
       "   'confident',\n",
       "   'content',\n",
       "   'devastated',\n",
       "   'disappointed',\n",
       "   'disgusted',\n",
       "   'embarrassed',\n",
       "   'excited',\n",
       "   'faithful',\n",
       "   'fear',\n",
       "   'furious',\n",
       "   'grateful',\n",
       "   'guilty',\n",
       "   'hopeful',\n",
       "   'impressed',\n",
       "   'jealous',\n",
       "   'joy',\n",
       "   'joyful',\n",
       "   'lonely',\n",
       "   'love',\n",
       "   'nostalgic',\n",
       "   'prepared',\n",
       "   'proud',\n",
       "   'sad',\n",
       "   'sadness',\n",
       "   'sentimental',\n",
       "   'surprise',\n",
       "   'surprised',\n",
       "   'terrified',\n",
       "   'trusting'],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-emotions.gguf']},\n",
       " {'model_name': 'slim-ratings-tool',\n",
       "  'display_name': 'slim-ratings-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-ratings.gguf',\n",
       "  'gguf_repo': 'llmware/slim-ratings-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-ratings-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['rating'],\n",
       "  'fc_output_values': ['1', '2', '3', '4', '5'],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-ratings.gguf']},\n",
       " {'model_name': 'slim-intent-tool',\n",
       "  'display_name': 'slim-intent-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-intent.gguf',\n",
       "  'gguf_repo': 'llmware/slim-intent-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-intent-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['intent'],\n",
       "  'fc_output_values': ['account',\n",
       "   'cancel',\n",
       "   'complaint',\n",
       "   'customer service',\n",
       "   'delivery',\n",
       "   'feedback',\n",
       "   'invoice',\n",
       "   'new account',\n",
       "   'order',\n",
       "   'payments',\n",
       "   'refund',\n",
       "   'shipping',\n",
       "   'subscription',\n",
       "   'terminate'],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-intent.gguf']},\n",
       " {'model_name': 'slim-nli-tool',\n",
       "  'display_name': 'slim-nli-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-nli.gguf',\n",
       "  'gguf_repo': 'llmware/slim-nli-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-nli-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['evidence'],\n",
       "  'fc_output_values': ['supports', 'neutral', 'contradicts'],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [9996, 5924, 17821],\n",
       "  'marker_token_lookup': {9996: 'contradicts',\n",
       "   5924: 'supports',\n",
       "   17821: 'neutral'},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-nli.gguf']},\n",
       " {'model_name': 'slim-topics-tool',\n",
       "  'display_name': 'slim-topics-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-topics.gguf',\n",
       "  'gguf_repo': 'llmware/slim-topics-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-topics-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['topics'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-topics.gguf']},\n",
       " {'model_name': 'slim-tags-tool',\n",
       "  'display_name': 'slim-tags-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-tags.gguf',\n",
       "  'gguf_repo': 'llmware/slim-tags-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-tags-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['tags'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-tags.gguf']},\n",
       " {'model_name': 'slim-sql-tool',\n",
       "  'display_name': 'slim-sql-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-sql.gguf',\n",
       "  'gguf_repo': 'llmware/slim-sql-tool',\n",
       "  'fc_output_values': [],\n",
       "  'link': 'https://huggingface.co/llmware/slim-sql-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'tokenizer': 'llmware/slim-sql-1b-v0',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-sql.gguf']},\n",
       " {'model_name': 'bling-answer-tool',\n",
       "  'display_name': 'bling-answer-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'bling-answer.gguf',\n",
       "  'gguf_repo': 'llmware/bling-answer-tool',\n",
       "  'link': 'https://huggingface.co/llmware/bling-answer-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'tokenizer': 'llmware/bling-tiny-llama-1b-v0',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['bling-answer.gguf']},\n",
       " {'model_name': 'slim-category-tool',\n",
       "  'display_name': 'slim-category-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-category.gguf',\n",
       "  'gguf_repo': 'llmware/slim-category-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-category-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['category'],\n",
       "  'fc_output_values': ['analyst',\n",
       "   'announcements',\n",
       "   'bonds',\n",
       "   'business',\n",
       "   'central bank',\n",
       "   'commentary',\n",
       "   'commodities',\n",
       "   'currencies',\n",
       "   'dividend',\n",
       "   'earnings',\n",
       "   'energy',\n",
       "   'entertainment',\n",
       "   'financials',\n",
       "   'health',\n",
       "   'human resources',\n",
       "   'legal and regulation',\n",
       "   'macroeconomics',\n",
       "   'markets',\n",
       "   'mergers and acquisitions',\n",
       "   'opinion',\n",
       "   'politics',\n",
       "   'public markets',\n",
       "   'science',\n",
       "   'sports',\n",
       "   'stocks',\n",
       "   'tech',\n",
       "   'world'],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-category.gguf']},\n",
       " {'model_name': 'llmware/slim-intent',\n",
       "  'display_name': 'slim-intent-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-intent',\n",
       "  'hf_repo': 'llmware/slim-intent',\n",
       "  'custom_model_files': [''],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['intent'],\n",
       "  'fc_output_values': ['account',\n",
       "   'cancel',\n",
       "   'complaint',\n",
       "   'customer service',\n",
       "   'delivery',\n",
       "   'feedback',\n",
       "   'invoice',\n",
       "   'new account',\n",
       "   'order',\n",
       "   'payments',\n",
       "   'refund',\n",
       "   'shipping',\n",
       "   'subscription',\n",
       "   'terminate'],\n",
       "  'function': ['classify'],\n",
       "  'marker_tokens': [1066, 22198, 17821],\n",
       "  'marker_token_lookup': {1066: 'positive',\n",
       "   22198: 'negative',\n",
       "   17821: 'neutral'}},\n",
       " {'model_name': 'llmware/slim-sentiment',\n",
       "  'display_name': 'slim-sentiment-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-sentiment',\n",
       "  'hf_repo': 'llmware/slim-sentiment',\n",
       "  'custom_model_files': [''],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['sentiment'],\n",
       "  'fc_output_values': ['positive', 'neutral', 'negative'],\n",
       "  'marker_tokens': [1066, 22198, 17821],\n",
       "  'marker_token_lookup': {1066: 'positive',\n",
       "   22198: 'negative',\n",
       "   17821: 'neutral'},\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'llmware/slim-emotions',\n",
       "  'display_name': 'slim-emotions-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-emotions',\n",
       "  'hf_repo': 'llmware/slim-emotions',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['emotions'],\n",
       "  'fc_output_values': ['afraid',\n",
       "   'anger',\n",
       "   'angry',\n",
       "   'annoyed',\n",
       "   'anticipating',\n",
       "   'anxious',\n",
       "   'apprehensive',\n",
       "   'ashamed',\n",
       "   'caring',\n",
       "   'confident',\n",
       "   'content',\n",
       "   'devastated',\n",
       "   'disappointed',\n",
       "   'disgusted',\n",
       "   'embarrassed',\n",
       "   'excited',\n",
       "   'faithful',\n",
       "   'fear',\n",
       "   'furious',\n",
       "   'grateful',\n",
       "   'guilty',\n",
       "   'hopeful',\n",
       "   'impressed',\n",
       "   'jealous',\n",
       "   'joy',\n",
       "   'joyful',\n",
       "   'lonely',\n",
       "   'love',\n",
       "   'nostalgic',\n",
       "   'prepared',\n",
       "   'proud',\n",
       "   'sad',\n",
       "   'sadness',\n",
       "   'sentimental',\n",
       "   'surprise',\n",
       "   'surprised',\n",
       "   'terrified',\n",
       "   'trusting'],\n",
       "  'marker_tokens': [1066, 22198, 17821],\n",
       "  'marker_token_lookup': {1066: 'positive',\n",
       "   22198: 'negative',\n",
       "   17821: 'neutral'},\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'llmware/slim-ner',\n",
       "  'display_name': 'slim-ner-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-ner',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'hf_repo': 'llmware/slim-ner',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['person', 'organization', 'place', 'misc'],\n",
       "  'fc_output_values': [],\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'llmware/slim-nli',\n",
       "  'display_name': 'slim-nli-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-nli',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/slim-nli',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['evidence'],\n",
       "  'fc_output_values': ['supports', 'neutral', 'contradicts'],\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'llmware/slim-ratings',\n",
       "  'display_name': 'slim-ratings-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-ratings',\n",
       "  'hf_repo': 'llmware/slim-ratings',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['rating'],\n",
       "  'fc_output_values': ['1', '2', '3', '4', '5'],\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'llmware/slim-category',\n",
       "  'display_name': 'slim-category-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-category',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'hf_repo': 'llmware/slim-category',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['category'],\n",
       "  'fc_output_values': ['analyst',\n",
       "   'announcements',\n",
       "   'bonds',\n",
       "   'business',\n",
       "   'central bank',\n",
       "   'commentary',\n",
       "   'commodities',\n",
       "   'currencies',\n",
       "   'dividend',\n",
       "   'earnings',\n",
       "   'energy',\n",
       "   'entertainment',\n",
       "   'financials',\n",
       "   'health',\n",
       "   'human resources',\n",
       "   'legal and regulation',\n",
       "   'macroeconomics',\n",
       "   'markets',\n",
       "   'mergers and acquisitions',\n",
       "   'opinion',\n",
       "   'politics',\n",
       "   'public markets',\n",
       "   'science',\n",
       "   'sports',\n",
       "   'stocks',\n",
       "   'tech',\n",
       "   'world'],\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'llmware/slim-tags',\n",
       "  'display_name': 'slim-tags-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-tags',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/slim-tags',\n",
       "  'outout_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['tags'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'llmware/slim-topics',\n",
       "  'display_name': 'slim-topics-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-topics',\n",
       "  'hf_repo': 'llmware/slim-topics',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['topics'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'llmware/slim-sql-1b-v0',\n",
       "  'display_name': 'slim-sql-1b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-sql-1b-v0',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'llmware/slim-sql-1b-v0',\n",
       "  'function_call': False,\n",
       "  'fc_output_values': [],\n",
       "  'primary_keys': ['sql'],\n",
       "  'function': ['sql']},\n",
       " {'model_name': 'bling-stablelm-3b-tool',\n",
       "  'display_name': 'llmware/bling-stablelm-3b-gguf',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'bling-stablelm.gguf',\n",
       "  'gguf_repo': 'llmware/bling-stablelm-3b-gguf',\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['bling-stablelm.gguf'],\n",
       "  'link': 'https://huggingface.co/llmware/bling-stablelm-3b-gguf',\n",
       "  'tokenizer_local': 'tokenizer_stablelm.json',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': ''},\n",
       " {'model_name': 'slim-xsum',\n",
       "  'display_name': 'llmware/slim-xsum',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-xsum',\n",
       "  'hf_repo': 'llmware/slim-xsum',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['xsum'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'slim-xsum-tool',\n",
       "  'display_name': 'slim-xsum-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-xsum.gguf',\n",
       "  'gguf_repo': 'llmware/slim-xsum-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-xsum-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['xsum'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-extract',\n",
       "  'tokenizer_local': 'tokenizer_stablelm.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-xsum.gguf']},\n",
       " {'model_name': 'slim-extract',\n",
       "  'display_name': 'llmware/slim-extract',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-extract',\n",
       "  'hf_repo': 'llmware/slim-extract',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['key data points'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['extract']},\n",
       " {'model_name': 'slim-extract-tiny',\n",
       "  'display_name': 'llmware/slim-extract-tiny',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-extract-tiny',\n",
       "  'hf_repo': 'llmware/slim-extract-tiny',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['key data points'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['extract']},\n",
       " {'model_name': 'slim-extract-tool',\n",
       "  'display_name': 'slim-extract-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-extract.gguf',\n",
       "  'gguf_repo': 'llmware/slim-extract-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-extract-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['key data points'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-extract',\n",
       "  'tokenizer_local': 'tokenizer_stablelm.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['extract'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-extract.gguf']},\n",
       " {'model_name': 'llmware/slim-extract-tiny-tool',\n",
       "  'display_name': 'slim-extract-tiny-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'tiny-extract.gguf',\n",
       "  'gguf_repo': 'llmware/slim-extract-tiny-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-extract-tiny-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['key points'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['tiny-extract.gguf']},\n",
       " {'model_name': 'llmware/slim-summary-tiny-tool',\n",
       "  'display_name': 'slim-summary-tiny-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'tiny-summary.gguf',\n",
       "  'gguf_repo': 'llmware/slim-summary-tiny-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-summary-tiny-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['key points'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['tiny-summary.gguf']},\n",
       " {'model_name': 'slim-boolean',\n",
       "  'display_name': 'llmware/slim-boolean',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-boolean',\n",
       "  'hf_repo': 'llmware/slim-boolean',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [2369, 9820],\n",
       "  'marker_token_lookup': {2369: 'no', 9820: 'yes'},\n",
       "  'primary_keys': [],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['boolean']},\n",
       " {'model_name': 'slim-boolean-tool',\n",
       "  'display_name': 'slim-boolean-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-boolean.gguf',\n",
       "  'gguf_repo': 'llmware/slim-boolean-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-boolean-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': [],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-extract',\n",
       "  'tokenizer_local': 'tokenizer_stablelm.json',\n",
       "  'marker_tokens': [2369, 9820],\n",
       "  'marker_token_lookup': {2369: 'no', 9820: 'yes'},\n",
       "  'function': ['boolean'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-boolean.gguf']},\n",
       " {'model_name': 'slim-sa-ner',\n",
       "  'display_name': 'llmware/slim-sa-ner',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-sa-ner',\n",
       "  'hf_repo': 'llmware/slim-sa-ner',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['sentiment, person, organization, place'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'slim-sa-ner-tool',\n",
       "  'display_name': 'slim-sa-ner-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'sa-ner.gguf',\n",
       "  'gguf_repo': 'llmware/slim-sa-ner-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-sa-ner-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['sentiment, person, organization, place'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-extract',\n",
       "  'tokenizer_local': 'tokenizer_stablelm.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['sa-ner.gguf']},\n",
       " {'model_name': 'slim-tags-3b',\n",
       "  'display_name': 'llmware/slim-tags-3b',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-tags-3b',\n",
       "  'hf_repo': 'llmware/slim-tags-3b',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['tags'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['classify']},\n",
       " {'model_name': 'slim-tags-3b-tool',\n",
       "  'display_name': 'slim-tags-3b-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-tags-3b.gguf',\n",
       "  'gguf_repo': 'llmware/slim-tags-3b-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-tags-3b-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['tags'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-extract',\n",
       "  'tokenizer_local': 'tokenizer_stablelm.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['classify'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-tags-3b.gguf']},\n",
       " {'model_name': 'slim-summary',\n",
       "  'display_name': 'llmware/slim-summary',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-summary',\n",
       "  'hf_repo': 'llmware/slim-summary',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'list',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['key points (3)'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['summarize']},\n",
       " {'model_name': 'slim-summary-tiny',\n",
       "  'display_name': 'llmware/slim-summary-tiny',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-summary-tiny',\n",
       "  'hf_repo': 'llmware/slim-summary-tiny',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'list',\n",
       "  'function_call': True,\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'primary_keys': ['key points (3)'],\n",
       "  'fc_output_values': [],\n",
       "  'function': ['summarize']},\n",
       " {'model_name': 'slim-summary-tool',\n",
       "  'display_name': 'slim-summary-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.0,\n",
       "  'sample_default': False,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'slim-summarize.gguf',\n",
       "  'gguf_repo': 'llmware/slim-summary-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-summary-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'list',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['key points (3)'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-extract',\n",
       "  'tokenizer_local': 'tokenizer_stablelm.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['summarize'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['slim-summarize.gguf'],\n",
       "  'standard': True},\n",
       " {'model_name': 'slim-q-gen-phi-3-tool',\n",
       "  'display_name': 'slim-q-gen-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'q_gen.gguf',\n",
       "  'gguf_repo': 'llmware/slim-q-gen-phi-3-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-q-gen-phi-3-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['question'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'microsoft/Phi-3-mini-4k-instruct',\n",
       "  'tokenizer_local': 'tokenizer_phi3.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['generate'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['q_gen.gguf']},\n",
       " {'model_name': 'slim-q-gen-tiny-tool',\n",
       "  'display_name': 'llmware/slim-q-gen-tiny-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.5,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'q_gen.gguf',\n",
       "  'gguf_repo': 'llmware/slim-q-gen-tiny-tool',\n",
       "  'link': 'https://huggingface.co/slim-q-gen-tiny-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['question'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['generate'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['q_gen.gguf']},\n",
       " {'model_name': 'llmware/slim-q-gen-tiny',\n",
       "  'display_name': 'slim-q-gen-tiny',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.5,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-q-gen-tiny',\n",
       "  'hf_repo': 'llmware/slim-q-gen-tiny',\n",
       "  'custom_model_files': [''],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['question'],\n",
       "  'fc_output_values': ['question'],\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['generate']},\n",
       " {'model_name': 'llmware/slim-q-gen-phi-3',\n",
       "  'display_name': 'slim-q-gen-phi-3',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.5,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-q-gen-phi-3',\n",
       "  'hf_repo': 'llmware/slim-q-gen-phi-3',\n",
       "  'custom_model_files': [''],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['question'],\n",
       "  'fc_output_values': ['question'],\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['generate']},\n",
       " {'model_name': 'slim-qa-gen-tiny-tool',\n",
       "  'display_name': 'llmware/slim-qa-gen-tiny-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.5,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'qa_gen_v3.gguf',\n",
       "  'gguf_repo': 'llmware/slim-qa-gen-tiny-tool',\n",
       "  'link': 'https://huggingface.co/slim-qa-gen-tiny-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['question, answer'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'llmware/slim-sentiment',\n",
       "  'tokenizer_local': 'tokenizer_tl.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['generate'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['qa_gen_v3.gguf']},\n",
       " {'model_name': 'slim-qa-gen-phi-3-tool',\n",
       "  'display_name': 'slim-qa-gen-phi-3-tool',\n",
       "  'model_family': 'GGUFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'llmware_repo',\n",
       "  'context_window': 4096,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.3,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': 'qa_gen_v3.gguf',\n",
       "  'gguf_repo': 'llmware/slim-qa-gen-phi-3-tool',\n",
       "  'link': 'https://huggingface.co/llmware/slim-qa-gen-phi-3-tool',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['question, answer'],\n",
       "  'fc_output_values': [],\n",
       "  'tokenizer': 'microsoft/Phi-3-mini-4k-instruct',\n",
       "  'tokenizer_local': 'tokenizer_phi3.json',\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['generate'],\n",
       "  'fetch': {'module': 'llmware.models', 'method': 'pull_snapshot_from_hf'},\n",
       "  'validation_files': ['qa_gen_v3.gguf']},\n",
       " {'model_name': 'llmware/slim-qa-gen-tiny',\n",
       "  'display_name': 'slim-qa-gen-tiny',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.5,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-qa-gen-tiny',\n",
       "  'hf_repo': 'llmware/slim-qa-gen-tiny',\n",
       "  'custom_model_files': [''],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['question, answer'],\n",
       "  'fc_output_values': ['question, answer'],\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['generate']},\n",
       " {'model_name': 'llmware/slim-qa-gen-phi-3',\n",
       "  'display_name': 'slim-qa-gen-phi-3',\n",
       "  'model_family': 'HFGenerativeModel',\n",
       "  'model_category': 'generative_local',\n",
       "  'model_location': 'hf_repo',\n",
       "  'context_window': 2048,\n",
       "  'instruction_following': False,\n",
       "  'prompt_wrapper': 'human_bot',\n",
       "  'temperature': 0.5,\n",
       "  'sample_default': True,\n",
       "  'trailing_space': '',\n",
       "  'gguf_file': '',\n",
       "  'gguf_repo': '',\n",
       "  'link': 'https://huggingface.co/llmware/slim-qa-gen-phi-3',\n",
       "  'hf_repo': 'llmware/slim-qa-gen-phi-3',\n",
       "  'custom_model_files': [''],\n",
       "  'custom_model_repo': '',\n",
       "  'output_type': 'dict',\n",
       "  'function_call': True,\n",
       "  'primary_keys': ['question, answer'],\n",
       "  'fc_output_values': ['question, answer'],\n",
       "  'marker_tokens': [],\n",
       "  'marker_token_lookup': {},\n",
       "  'function': ['generate']},\n",
       " {'model_name': 'jinaai/jina-reranker-v1-turbo-en',\n",
       "  'display_name': 'jina-reranker-turbo',\n",
       "  'model_family': 'HFReRankerModel',\n",
       "  'model_category': 'reranker',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 384,\n",
       "  'context_window': 8192,\n",
       "  'link': 'https://huggingface.co/jinaai/jina-reranker-v1-turbo-en',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'jinaai/jina-reranker-v1-turbo-en'},\n",
       " {'model_name': 'jinaai/jina-reranker-v1-tiny-en',\n",
       "  'display_name': 'jina-reranker-tiny',\n",
       "  'model_family': 'HFReRankerModel',\n",
       "  'model_category': 'reranker',\n",
       "  'model_location': 'hf_repo',\n",
       "  'embedding_dims': 384,\n",
       "  'context_window': 8192,\n",
       "  'link': 'https://huggingface.co/jinaai/jina-reranker-v1-tiny-en',\n",
       "  'custom_model_files': [],\n",
       "  'custom_model_repo': '',\n",
       "  'hf_repo': 'jinaai/jina-reranker-v1-tiny-en'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmware.models import ModelCatalog\n",
    "catalog = ModelCatalog()\n",
    "catalog.list_all_models()\n",
    "#catalog.get_llm_toolkit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: JD for .Net Position 2.docx\n",
      "Model: llmware/bling-1b-0.1\n",
      "LLM Response:  Software Engineering Manager, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft Corporation, Microsoft\n",
      "Time taken: 94.19 seconds\n",
      "\n",
      "File: JD for .Net Position 2.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Advanced software engineering using modern software languages and architecture patterns\n",
      "•Up-to-date knowledge of current trends in software development\n",
      "•Developing new enterprise web services in REST using the latest patterns\n",
      "•Developing various database schemas and complex SQL queries\n",
      "•Maintaining and resolving issues in a highly complex integration architecture\n",
      "•Supporting various testing cycles including unit testing, integration, UAT & regression testing\n",
      "•Leveraging CI/CD pipelines as part of DevOps platforms to deliver code into test and production\n",
      "•Following up with application owners to resolve issues as needed\n",
      "•Performing development-tier production support and software release activities within a team rotation\n",
      "•Providing status on assignment tasks/daily standup tasks\n",
      "•Participating in and being a member of a multi-location, global software development team\n",
      "•Mentoring others in various technical\n",
      "Time taken: 128.59 seconds\n",
      "\n",
      "File: JD for .Net Position 2.docx\n",
      "Model: llmware/bling-falcon-1b-0.1\n",
      "LLM Response: \n",
      "The role is to develop and deliver business-focused solutions to the\n",
      "organization.\n",
      "The role is to develop and deliver business-focused solutions to the\n",
      "organization.\n",
      "The role is to develop and deliver business-focused solutions to the\n",
      "organization.\n",
      "The role is to develop and deliver business-focused solutions to the\n",
      "organization.\n",
      "The role is to develop and deliver business-focused solutions to the\n",
      "organization.\n",
      "The role is to develop and deliver business-focused solutions to the\n",
      "organization.\n",
      "The role is to develop and deliver business-focused solutions to the\n",
      "organization.\n",
      "The role is to develop and deliver business-focused solutions to the\n",
      "organization.\n",
      "The role is to develop and deliver business-focused\n",
      "The role is to\n",
      "The role is to\n",
      "The role is to\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "Time taken: 173.84 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: JD Native Hana 1.docx\n",
      "Model: llmware/bling-1b-0.1\n",
      "LLM Response:  SAP Native Hana, SQL, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical Expertise, Technical\n",
      "Time taken: 76.77 seconds\n",
      "\n",
      "File: JD Native Hana 1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Mandatory Skills : SAP Native Hana ,SQL\n",
      "Time taken: 13.29 seconds\n",
      "\n",
      "File: JD Native Hana 1.docx\n",
      "Model: llmware/bling-falcon-1b-0.1\n",
      "LLM Response:  The job description is as follows:                                                                                                                                                                                                 \n",
      "Time taken: 113.34 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: JDFirewall P4 1.docx\n",
      "Model: llmware/bling-1b-0.1\n",
      "LLM Response:  Senior Information Security Engineer position\n",
      "Time taken: 17.0 seconds\n",
      "\n",
      "File: JDFirewall P4 1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Senior Information Security Engineer\n",
      "Time taken: 37.44 seconds\n",
      "\n",
      "File: JDFirewall P4 1.docx\n",
      "Model: llmware/bling-falcon-1b-0.1\n",
      "LLM Response:  Senior Information Security Engineer\n",
      "Time taken: 142.64 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Oracle DBA_V1.docx\n",
      "Model: llmware/bling-1b-0.1\n",
      "LLM Response:  Database Admin Consultant II\n",
      "Time taken: 9.4 seconds\n",
      "\n",
      "File: Oracle DBA_V1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Oracle Instance maintenance, \n",
      "•8-10 years of experience in Oracle Instance maintenance and troubleshooting role\n",
      "•Good with standard DBA activities like installation, configuration and troubleshooting issues\n",
      "•Hardening of databases and implementing database best practices\n",
      "•Experience with Database refresh and clone activities\n",
      "•Strong Knowledge of Database Backup and recovery using RMAN and logical backups.\n",
      "•Ability to collaborate independently on critical issues with clients and provide root cause analysis\n",
      "•Worked with ADDM/ASH/AWR reports and performance tuning.\n",
      "•Good understanding of 3-tier application architecture\n",
      "•Good communication skills\n",
      "•Basic RedHat/Linux Administration\n",
      "•Basic scripting skills with Unix Shell Scripting, PowerShell, Python\n",
      "•Understanding of OEM and Oracle grid-control.\n",
      "•Knowledge of Oracle EBS, SQL Server and administration is a plus\n",
      "•Development\n",
      "Time taken: 91.02 seconds\n",
      "\n",
      "File: Oracle DBA_V1.docx\n",
      "Model: llmware/bling-falcon-1b-0.1\n",
      "LLM Response:  The Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers.                                                                                                                                                                \n",
      "Time taken: 114.33 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.models import ModelCatalog\n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing your .docx files\n",
    "input_dir = r'C:\\DataScience-TechERG\\LLM_JD\\Input_Files'\n",
    "\n",
    "# List of models to use\n",
    "models = [\"llmware/bling-1b-0.1\", \"llmware/bling-tiny-llama-v0\", \"llmware/bling-falcon-1b-0.1\"]\n",
    "\n",
    "# Initialize the prompter\n",
    "prompter = Prompt()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.docx') and not file_name.startswith('~$'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # Read the document content\n",
    "        doc_content = read_docx(file_path)\n",
    "        \n",
    "        # Define the prompt\n",
    "        prompt = \"Summarize the job description\"\n",
    "        context = doc_content  # Using the content read from the .docx file as the context\n",
    "        \n",
    "        for model_name in models:\n",
    "            # Load the model from the catalog\n",
    "            prompter.load_model(model_name)\n",
    "            \n",
    "            # Record start time for operation\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate a response using the prompter\n",
    "            response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "            \n",
    "            # Calculate and print the time taken for the operation\n",
    "            time_taken = round(time.time() - start_time, 2)\n",
    "            \n",
    "            # Extract 'llm_response' from the response and print it\n",
    "            llm_response = response.get('llm_response', 'Response not found')\n",
    "            print(f\"File: {file_name}\")\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"LLM Response: {llm_response}\")\n",
    "            print(f\"Time taken: {time_taken} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: JD for .Net Position 2.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Advanced software engineering using modern software languages and architecture patterns\n",
      "•Up-to-date knowledge of current trends in software development\n",
      "•Developing new enterprise web services in REST using the latest patterns\n",
      "•Developing various database schemas and complex SQL queries\n",
      "•Maintaining and resolving issues in a highly complex integration architecture\n",
      "•Supporting various testing cycles including unit testing, integration, UAT & regression testing\n",
      "•Leveraging CI/CD pipelines as part of DevOps platforms to deliver code into test and production\n",
      "•Following up with application owners to resolve issues as needed\n",
      "•Performing development-tier production support and software release activities within a team rotation\n",
      "•Providing status on assignment tasks/daily standup tasks\n",
      "•Participating in and being a member of a multi-location, global software development team\n",
      "•Mentoring others in various technical\n",
      "Time taken: 125.1 seconds\n",
      "\n",
      "File: JD for .Net Position 2.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['The main responsibilities include  Advanced software engineering using modern software languages and architecture patterns', 'Up-to-date knowledge of current trends in software development', 'Developing new enterprise web services in REST using the latest patterns', 'Developing various database schemas and complex SQL queries', 'Maintaining and resolving issues in a highly complex integration architecture', 'Supporting various testing cycles including unit testing integration UAT & regression testing', 'Leveraging CI/CD pipelines as part of DevOps platforms to deliver code into test and production', 'Following up with application owners to resolve issues as needed', 'Participating in and being a member of a multi-location global software development team', 'Providing status on assignment tasks/daily standup tasks']\n",
      "Time taken: 153.73 seconds\n",
      "\n",
      "File: JD Native Hana 1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Mandatory Skills : SAP Native Hana ,SQL\n",
      "Time taken: 10.34 seconds\n",
      "\n",
      "File: JD Native Hana 1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['SAP HANA, SQL, XS ODATA, XS JS, HANA Performance Tuning, Migration of HANA content objects']\n",
      "Time taken: 27.91 seconds\n",
      "\n",
      "File: JDFirewall P4 1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Senior Information Security Engineer\n",
      "Time taken: 24.32 seconds\n",
      "\n",
      "File: JDFirewall P4 1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: []\n",
      "Time taken: 105.03 seconds\n",
      "\n",
      "File: Oracle DBA_V1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Oracle Instance maintenance, \n",
      "•8-10 years of experience in Oracle Instance maintenance and troubleshooting role\n",
      "•Good with standard DBA activities like installation, configuration and troubleshooting issues\n",
      "•Hardening of databases and implementing database best practices\n",
      "•Experience with Database refresh and clone activities\n",
      "•Strong Knowledge of Database Backup and recovery using RMAN and logical backups.\n",
      "•Ability to collaborate independently on critical issues with clients and provide root cause analysis\n",
      "•Worked with ADDM/ASH/AWR reports and performance tuning.\n",
      "•Good understanding of 3-tier application architecture\n",
      "•Good communication skills\n",
      "•Basic RedHat/Linux Administration\n",
      "•Basic scripting skills with Unix Shell Scripting, PowerShell, Python\n",
      "•Understanding of OEM and Oracle grid-control.\n",
      "•Knowledge of Oracle EBS, SQL Server and administration is a plus\n",
      "•Development\n",
      "Time taken: 86.32 seconds\n",
      "\n",
      "File: Oracle DBA_V1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Database Admin Consultant II', 'Reporting to Lead DBA/Manager', 'Location Bangalore India', 'Description The Senior Database Admin Consultant DBA will be a member of the Global Database Engineering team and will be responsible for effective service transition and efficient service delivery operations for database environments for our managed hosting customers', 'Required skills Oracle Instance maintenance 8-10 years of experience in Oracle Instance maintenance and troubleshooting role Good with standard DBA activities like installation configuration and troubleshooting issues Hardening of databases and implementing database best practices Experience with Database Refresh and Clone activities Strong Knowledge of Database Backup and recovery using RMAN and logical backups', 'Ability to collaborate independently on critical issues with clients and provide root cause analysis', 'Worked with ADDM/ASH/AWR reports and performance tuning', 'Good communication skills']\n",
      "Time taken: 66.5 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing your .docx files\n",
    "input_dir = r'C:\\DataScience-TechERG\\LLM_JD\\Input_Files'\n",
    "\n",
    "# List of models to use\n",
    "models = [ \"llmware/bling-tiny-llama-v0\",\"slim-summary-tool\"]\n",
    "\n",
    "# Initialize the prompter\n",
    "prompter = Prompt()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.docx') and not file_name.startswith('~$'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # Read the document content\n",
    "        doc_content = read_docx(file_path)\n",
    "        \n",
    "        # Define the prompt\n",
    "        prompt = \"Summarize the job description\"\n",
    "        context = doc_content  # Using the content read from the .docx file as the context\n",
    "        \n",
    "        for model_name in models:\n",
    "            # Load the model from the catalog\n",
    "            prompter.load_model(model_name)\n",
    "            \n",
    "            # Record start time for operation\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate a response using the prompter\n",
    "            response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "            \n",
    "            # Calculate and print the time taken for the operation\n",
    "            time_taken = round(time.time() - start_time, 2)\n",
    "            \n",
    "            # Extract 'llm_response' from the response and print it\n",
    "            llm_response = response.get('llm_response', 'Response not found')\n",
    "            print(f\"File: {file_name}\")\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"LLM Response: {llm_response}\")\n",
    "            print(f\"Time taken: {time_taken} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['all-MiniLM-L6-v2', 'all-mpnet-base-v2', 'industry-bert-insurance', 'industry-bert-contracts', 'industry-bert-asset-management', 'industry-bert-sec', 'industry-bert-loans', 'nomic-ai/nomic-embed-text-v1', 'jinaai/jina-embeddings-v2-base-en', 'jinaai/jina-embeddings-v2-small-en', 'BAAI/bge-small-en-v1.5', 'BAAI/bge-large-en-v1.5', 'BAAI/bge-base-en-v1.5', 'thenlper/gte-small', 'thenlper/gte-base', 'thenlper/gte-large', 'llmrails/ember-v1', 'WhereIsAI/UAE-Large-V1', 'text-embedding-ada-002', 'text-embedding-3-small', 'text-embedding-3-large', 'medium', 'xlarge', 'embed-english-v3.0', 'embed-multilingual-v3.0', 'embed-english-light-v3.0', 'embed-multilingual-light-v3.0', 'embed-english-v2.0', 'embed-english-light-v2.0', 'embed-multilingual-v2.0', 'textembedding-gecko@latest', 'claude-v1', 'claude-instant-v1', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-2.1', 'claude-2.0', 'command-medium-nightly', 'command-xlarge-nightly', 'summarize-xlarge', 'summarize-medium', 'j2-jumbo-instruct', 'j2-grande-instruct', 'text-bison@001', 'chat-bison@001', 'text-davinci-003', 'text-curie-001', 'text-babbage-001', 'text-ada-001', 'gpt-3.5-turbo', 'gpt-4', 'gpt-3.5-turbo-instruct', 'gpt-4-1106-preview', 'gpt-3.5-turbo-1106', 'gpt-4-0125-preview', 'gpt-3.5-turbo-0125', 'gpt-4o', 'gpt-4o-2024-05-13', 'llmware-inference-server', 'llmware/bling-1.4b-0.1', 'llmware/bling-1b-0.1', 'llmware/bling-falcon-1b-0.1', 'llmware/bling-sheared-llama-1.3b-0.1', 'llmware/bling-red-pajamas-3b-0.1', 'llmware/bling-sheared-llama-2.7b-0.1', 'llmware/bling-stable-lm-3b-4e1t-v0', 'llmware/bling-cerebras-1.3b-0.1', 'llmware/bling-tiny-llama-v0', 'llmware/dragon-yi-6b-v0', 'llmware/dragon-stablelm-7b-v0', 'llmware/dragon-mistral-7b-v0', 'llmware/dragon-red-pajama-7b-v0', 'llmware/dragon-deci-6b-v0', 'llmware/dragon-falcon-7b-v0', 'llmware/dragon-llama-7b-v0', 'llmware/dragon-deci-7b-v0', 'llmware/bling-phi-3', 'bling-phi-3-gguf', 'llmware/dragon-mistral-7b-gguf', 'llmware/dragon-llama-7b-gguf', 'llmware/dragon-yi-6b-gguf', 'dragon-yi-answer-tool', 'dragon-llama-answer-tool', 'dragon-mistral-answer-tool', 'TheBloke/Llama-2-7B-Chat-GGUF', 'TheBloke/OpenHermes-2.5-Mistral-7B-GGUF', 'TheBloke/zephyr-7B-beta-GGUF', 'TheBloke/Starling-LM-7B-alpha-GGUF', 'microsoft/Phi-3-mini-4k-instruct-gguf', 'microsoft/Phi-3-mini-4k-instruct', 'microsoft/Phi-3-mini-128k-instruct', 'Meta-Llama-3-8B-Instruct', 'Meta-Llama-3-8B', 'QuantFactory/Meta-Llama-3-8B-Instruct-GGUF', 'QuantFactory/Meta-Llama-3-8B-GGUF', 'bartowski/Meta-Llama-3-8B-Instruct-GGUF', 'tiny-llama-chat-gguf', 'whisper-cpp-base-english', 'whisper-cpp-base', 'whisper-cpp-tiny-diarize', 'slim-ner-tool', 'slim-sentiment-tool', 'slim-emotions-tool', 'slim-ratings-tool', 'slim-intent-tool', 'slim-nli-tool', 'slim-topics-tool', 'slim-tags-tool', 'slim-sql-tool', 'bling-answer-tool', 'slim-category-tool', 'llmware/slim-intent', 'llmware/slim-sentiment', 'llmware/slim-emotions', 'llmware/slim-ner', 'llmware/slim-nli', 'llmware/slim-ratings', 'llmware/slim-category', 'llmware/slim-tags', 'llmware/slim-topics', 'llmware/slim-sql-1b-v0', 'bling-stablelm-3b-tool', 'slim-xsum', 'slim-xsum-tool', 'slim-extract', 'slim-extract-tiny', 'slim-extract-tool', 'llmware/slim-extract-tiny-tool', 'llmware/slim-summary-tiny-tool', 'slim-boolean', 'slim-boolean-tool', 'slim-sa-ner', 'slim-sa-ner-tool', 'slim-tags-3b', 'slim-tags-3b-tool', 'slim-summary', 'slim-summary-tiny', 'slim-summary-tool', 'slim-q-gen-phi-3-tool', 'slim-q-gen-tiny-tool', 'llmware/slim-q-gen-tiny', 'llmware/slim-q-gen-phi-3', 'slim-qa-gen-tiny-tool', 'slim-qa-gen-phi-3-tool', 'llmware/slim-qa-gen-tiny', 'llmware/slim-qa-gen-phi-3', 'jinaai/jina-reranker-v1-turbo-en', 'jinaai/jina-reranker-v1-tiny-en']\n",
      "Debug: llmware/bling-tiny-llama-v0 is available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.HFGenerativeModel object at 0x0000026619F0DEE0>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file ~$Firewall P4 1.docx with model llmware/bling-tiny-llama-v0: '<llmware.models.HFGenerativeModel object at 0x0000026619F0DEE0>' could not be located\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.models import ModelCatalog\n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    " doc = Document(file_path)\n",
    " full_text = [para.text for para in doc.paragraphs]\n",
    " return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing your .docx files\n",
    "input_dir = r'C:\\DataScience-TechERG\\LLM_JD\\Input_Files'\n",
    "\n",
    "# List of models to use\n",
    "models = [ \"llmware/bling-tiny-llama-v0\"]\n",
    "\n",
    "# Initialize the ModelCatalog\n",
    "model_catalog = ModelCatalog()\n",
    "\n",
    "\n",
    "# Check available models in ModelCatalog\n",
    "available_models = model_catalog.list_all_models()\n",
    "print(\"Available models:\", [model['model_name'] for model in available_models])\n",
    "\n",
    "#model_name_to_check = \"all-MiniLM-L6-v2\".lower().strip()\n",
    "available_model_names = [model['model_name'].lower().strip() for model in available_models]\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for modelname in models:\n",
    "            try:\n",
    "                # Normalize model_name for case-insensitive comparison\n",
    "                normalized_model_name = model_name.lower().strip()\n",
    "                \n",
    "                # Check if the model is available\n",
    "                if normalized_model_name in available_model_names:\n",
    "                    print(f\"Debug: {model_name} is available.\")\n",
    "                    \n",
    "                    # Load the model from the catalog\n",
    "                    model_name = model_catalog.load_model(modelname)\n",
    "                    \n",
    "                    # Ensure the prompter is set to use the loaded model\n",
    "                    prompter = Prompt().load_model(model_name)\n",
    "                    #prompter.load_model(model)\n",
    "                    \n",
    "                    # Record start time for operation\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Generate a response using the prompter\n",
    "                    response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "                    \n",
    "                    # Calculate and print the time taken for the operation\n",
    "                    time_taken = round(time.time() - start_time, 2)\n",
    "                    \n",
    "                    # Extract 'llm_response' from the response and print it\n",
    "                    llm_response = response.get('llm_response', 'Response not found')\n",
    "                    print(f\"File: {file_name}\")\n",
    "                    print(f\"Model: {modelname}\")\n",
    "                    print(f\"LLM Response: {llm_response}\")\n",
    "                    print(f\"Time taken: {time_taken} seconds\\n\")\n",
    "                else:\n",
    "                    print(f\"Debug: {modelname} is not available.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name} with model {modelname}: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.HFGenerativeModel object at 0x0000026623F04430>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model llmware/bling-tiny-llama-v0 for file JD for .Net Position 2.docx. Error: '<llmware.models.HFGenerativeModel object at 0x0000026623F04430>' could not be located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.GGUFGenerativeModel object at 0x0000026619F6F7C0>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model slim-summary-tool for file JD for .Net Position 2.docx. Error: '<llmware.models.GGUFGenerativeModel object at 0x0000026619F6F7C0>' could not be located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.HFGenerativeModel object at 0x0000026618BDCD30>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model llmware/bling-tiny-llama-v0 for file JD Native Hana 1.docx. Error: '<llmware.models.HFGenerativeModel object at 0x0000026618BDCD30>' could not be located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.GGUFGenerativeModel object at 0x00000266244086D0>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model slim-summary-tool for file JD Native Hana 1.docx. Error: '<llmware.models.GGUFGenerativeModel object at 0x00000266244086D0>' could not be located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.HFGenerativeModel object at 0x0000026618D25460>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model llmware/bling-tiny-llama-v0 for file JDFirewall P4 1.docx. Error: '<llmware.models.HFGenerativeModel object at 0x0000026618D25460>' could not be located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.GGUFGenerativeModel object at 0x0000026623F04E80>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model slim-summary-tool for file JDFirewall P4 1.docx. Error: '<llmware.models.GGUFGenerativeModel object at 0x0000026623F04E80>' could not be located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.HFGenerativeModel object at 0x0000026619E87610>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model llmware/bling-tiny-llama-v0 for file Oracle DBA_V1.docx. Error: '<llmware.models.HFGenerativeModel object at 0x0000026619E87610>' could not be located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mERROR: error: ModelCatalog - unexpected - could not identify model card for selected model - <llmware.models.GGUFGenerativeModel object at 0x0000026619E874F0>\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model slim-summary-tool for file Oracle DBA_V1.docx. Error: '<llmware.models.GGUFGenerativeModel object at 0x0000026619E874F0>' could not be located\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.models import ModelCatalog\n",
    "\n",
    "def read_docx(file_path):\n",
    "    # Open the .docx file and extract all text.\n",
    "    doc = Document(file_path)\n",
    "    full_text = [para.text for para in doc.paragraphs]\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Set the directory where the .docx files are stored.\n",
    "input_dir = r'C:\\DataScience-TechERG\\LLM_JD\\Input_Files'\n",
    "\n",
    "# Define the models to be used for summarization.\n",
    "models = [\"llmware/bling-tiny-llama-v0\", \"slim-summary-tool\"]\n",
    "\n",
    "# Initialize the Prompt and ModelCatalog objects.\n",
    "prompter = Prompt()\n",
    "model_catalog = ModelCatalog()\n",
    "\n",
    "# Process each .docx file in the directory.\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.docx') and not file_name.startswith('~$'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # Extract text from the document.\n",
    "        doc_content = read_docx(file_path)\n",
    "        \n",
    "        # Set the prompt for summarization.\n",
    "        prompt = \"Summarize the job description\"\n",
    "        context = doc_content\n",
    "        \n",
    "        for model_name in models:\n",
    "            try:\n",
    "                loaded_model = model_catalog.load_model(model_name)\n",
    "                prompter.load_model(loaded_model)\n",
    "                start_time = time.time()\n",
    "                response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "                time_taken = round(time.time() - start_time, 2)\n",
    "                llm_response = response.get('llm_response', 'Response not found')\n",
    "                print(f\"File: {file_name}\")\n",
    "                print(f\"Model: {loaded_model}\")\n",
    "                print(f\"LLM Response: {llm_response}\")\n",
    "                print(f\"Time taken: {time_taken} seconds\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load model {model_name} for file {file_name}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: JD for .Net Position 2.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •.NET, SQL, Rest/SOAP API and Webservices and Angular 15\n",
      "•Knowledge in Enterprise Product/Service Configuration and Pricing\n",
      "•Self-starter, can-do attitude a must in a fast-moving business and technical environment.\n",
      "•Strong problem-solving skills, time management and oral and written communication skills\n",
      "•Must have skills:\n",
      "•Experience with developing for below technologies: -\n",
      "•C#\n",
      "•ADO.NET\n",
      "•.NET MVC\n",
      "•.NET Core\n",
      "•Web API\n",
      "•Webservices and Rest/SOAP\n",
      "•Angular 15\n",
      "•SQL Server\n",
      "•JavaScript\n",
      "•.NET Framework – Concepts\n",
      "•LINQ\n",
      "•Entity Framework\n",
      "•.NET – Nunit\n",
      "•Jquery\n",
      "•Optional Skills (Good To Have):\n",
      "\n",
      "Time taken: 122.91 seconds\n",
      "\n",
      "File: JD for .Net Position 2.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['C#', 'ADO.NET', '.NET MVC', '.NET Core', 'Web API', 'Webservices and Rest/SOAP', 'Angular 15', 'SQL Server', 'JavaScript', '.NET Framework - Concepts', 'LINQ', 'Entity Framework', '.NET - Nunit', 'Jquery']\n",
      "Time taken: 117.83 seconds\n",
      "\n",
      "File: JD Native Hana 1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •SAP Native Hana ,SQL\n",
      "Time taken: 7.65 seconds\n",
      "\n",
      "File: JD Native Hana 1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['SAP Native Hana', 'SQL']\n",
      "Time taken: 21.6 seconds\n",
      "\n",
      "File: JDFirewall P4 1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Fortinet, Firewall. •Palo alto, cisco, checkpoint, BGP routing.\n",
      "Time taken: 30.38 seconds\n",
      "\n",
      "File: JDFirewall P4 1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Fortinet', 'Firewall']\n",
      "Time taken: 73.49 seconds\n",
      "\n",
      "File: Oracle DBA_V1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Oracle Instance maintenance, \n",
      "•8-10 years of experience in Oracle Instance maintenance and troubleshooting role\n",
      "•Good with standard DBA activities like installation, configuration and troubleshooting issues\n",
      "•Hardening of databases and implementing database best practices\n",
      "•Experience with Database refresh and clone activities\n",
      "•Strong Knowledge of Database Backup and recovery using RMAN and logical backups.\n",
      "•Ability to collaborate independently on critical issues with clients and provide root cause analysis\n",
      "•Worked with ADDM/ASH/AWR reports and performance tuning.\n",
      "•Good understanding of 3-tier application architecture\n",
      "•Good communication skills\n",
      "•Basic RedHat/Linux Administration\n",
      "•Basic scripting skills with Unix Shell Scripting, PowerShell, Python\n",
      "•Understanding of OEM and Oracle grid-control.\n",
      "•Knowledge of Oracle EBS, SQL Server and administration is a plus\n",
      "•Development\n",
      "Time taken: 91.88 seconds\n",
      "\n",
      "File: Oracle DBA_V1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Oracle Instance maintenance', '8-10 years of experience in Oracle Instance maintenance and troubleshooting role', 'Good with standard DBA activities like installation, configuration and troubleshooting issues', 'Hardening of databases and implementing database best practices', 'Experience with Database Refresh and Clone activities', 'Strong Knowledge of Database Backup and recovery using RMAN and Logical Backups', 'Ability to collaborate independently on critical issues with clients and provide Root Cause Analysis', 'Worked with ADDM/ASH/AWR reports and performance tuning', 'Good understanding of 3-tier application architecture', 'Good communication skills', 'Basic Red Hat/Linux Administration', 'Basic scripting skills with Unix Shell Scripting, PowerShell, Python', 'Understanding of OEM and Oracle Grid-Control', 'Knowledge of Oracle EBS, SQL Server and Administration is a plus']\n",
      "Time taken: 60.39 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing your .docx files\n",
    "input_dir = r'C:\\DataScience-TechERG\\LLM_JD\\Input_Files'\n",
    "\n",
    "# List of models to use\n",
    "models = [ \"llmware/bling-tiny-llama-v0\",\"slim-summary-tool\"]\n",
    "\n",
    "# Initialize the prompter\n",
    "prompter = Prompt()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.docx') and not file_name.startswith('~$'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # Read the document content\n",
    "        doc_content = read_docx(file_path)\n",
    "        \n",
    "        # Define the prompt\n",
    "        prompt = \"Extract Mandatory or Primary Skills or Skill Set\"\n",
    "        context = doc_content  # Using the content read from the .docx file as the context\n",
    "        \n",
    "        for model_name in models:\n",
    "            # Load the model from the catalog\n",
    "            prompter.load_model(model_name)\n",
    "            \n",
    "            # Record start time for operation\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate a response using the prompter\n",
    "            response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "            \n",
    "            # Calculate and print the time taken for the operation\n",
    "            time_taken = round(time.time() - start_time, 2)\n",
    "            \n",
    "            # Extract 'llm_response' from the response and print it\n",
    "            llm_response = response.get('llm_response', 'Response not found')\n",
    "            print(f\"File: {file_name}\")\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"LLM Response: {llm_response}\")\n",
    "            print(f\"Time taken: {time_taken} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Java_P3_IAA_JD.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Java, J2EE, Spring, SpringBoot, Tomcat, SOAP, SQL, PL/SQL, Shell Scripting, Git, Maven, GitHub Copilot, AppDynamics, Splunk, SNCR tool, VFO, EP, BRMS and Orchestrator\n",
      "Time taken: 39.8 seconds\n",
      "\n",
      "File: Java_P3_IAA_JD.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Java', 'J2EE', 'Spring', 'SpringBoot', 'Tomcat', 'SOAP', 'SQL', 'PL/SQL', 'Shell Scripting', 'Git', 'Maven']\n",
      "Time taken: 63.36 seconds\n",
      "\n",
      "File: Java_P3_IS_ASR_JD.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Java, SpringBoot, Spring MVC, JAXRS, Tomcat, Rest, SOAP, SQL, Shell Scripting, Git, Maven, GitHub Copilot, AppDynamics, Splunk, Kafka, BigPanda, Vue, Angular, JQuery\n",
      "Time taken: 38.07 seconds\n",
      "\n",
      "File: Java_P3_IS_ASR_JD.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Java', 'Spring Boot', 'Spring MVC', 'JAXRS', 'Tomcat', 'REST', 'SOAP', 'SQL', 'Shell Scripting', 'Git', 'Maven']\n",
      "Time taken: 65.62 seconds\n",
      "\n",
      "File: Java_P3_JD (1).docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Java, SpringBoot, Spring MVC, JAXRS, Tomcat, Rest, SOAP, SQL, Shell Scripting, Git, Maven, GitHub Copilot, AppDynamics, Splunk, Kafka, BigPanda, Vue, Angular, JQuery\n",
      "Time taken: 37.28 seconds\n",
      "\n",
      "File: Java_P3_JD (1).docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Java', 'Spring Boot', 'Spring MVC', 'JAXRS', 'Tomcat', 'REST', 'SOAP', 'SQL', 'Shell Scripting', 'Git', 'Maven']\n",
      "Time taken: 64.41 seconds\n",
      "\n",
      "File: Java_P4_JD.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Java, SpringBoot, Spring MVC, JAXRS, Tomcat, Rest, SOAP, SQL, Shell Scripting, Vue, Angular, Git, Maven, GitHub Copilot, AppDynamics, Splunk, Kafka, BigPanda, JQuery\n",
      "Time taken: 36.22 seconds\n",
      "\n",
      "File: Java_P4_JD.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Java', 'Spring Boot', 'Spring MVC', 'JAXRS', 'Tomcat', 'REST', 'SOAP', 'SQL', 'Shell Scripting', 'Vue', 'Angular', 'Git', 'Maven']\n",
      "Time taken: 66.53 seconds\n",
      "\n",
      "File: JD for .Net Position 2.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •.NET, SQL, Rest/SOAP API and Webservices and Angular 15\n",
      "•Knowledge in Enterprise Product/Service Configuration and Pricing\n",
      "•Experience with developing for below technologies: -\n",
      "•C#\n",
      "•ADO.NET\n",
      "•.NET MVC\n",
      "•.NET Core\n",
      "•Web API\n",
      "•Webservices and Rest/SOAP\n",
      "•Angular 15\n",
      "•SQL Server\n",
      "•JavaScript\n",
      "•.NET Framework – Concepts\n",
      "•LINQ\n",
      "•Entity Framework\n",
      "•.NET – Nunit\n",
      "•Jquery\n",
      "•Optional Skills (Good To Have):\n",
      "•HTML5\n",
      "•CSS 3.0\n",
      "•Any UI Framework\n",
      "•C#\n",
      "•ADO.NET\n",
      "•.NET MVC\n",
      "•.NET Core\n",
      "•Web API\n",
      "•\n",
      "Time taken: 124.43 seconds\n",
      "\n",
      "File: JD for .Net Position 2.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['C#', 'ADO.NET', '.NET MVC', '.NET Core', 'Web API', 'Webservices and Rest/SOAP', 'Angular 15', 'SQL Server', 'JavaScript', '.NET Framework - Concepts', 'LINQ', 'Entity Framework', '.NET - Nunit', 'Jquery']\n",
      "Time taken: 158.65 seconds\n",
      "\n",
      "File: JD Native Hana 1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •SAP Native Hana ,SQL\n",
      "Time taken: 7.35 seconds\n",
      "\n",
      "File: JD Native Hana 1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['SAP Native Hana', 'SQL']\n",
      "Time taken: 19.84 seconds\n",
      "\n",
      "File: JD Native Hana 2.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •SAP Native Hana ,SQL\n",
      "Time taken: 7.04 seconds\n",
      "\n",
      "File: JD Native Hana 2.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['SAP Native Hana', 'SQL']\n",
      "Time taken: 20.11 seconds\n",
      "\n",
      "File: JD Native Hana.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: SAP Native Hana ,SQL\n",
      "Time taken: 3.93 seconds\n",
      "\n",
      "File: JD Native Hana.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['SAP Native Hana', 'SQL']\n",
      "Time taken: 5.91 seconds\n",
      "\n",
      "File: JDFirewall P4 1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Fortinet, Firewall. •Palo alto, cisco, checkpoint, BGP routing.\n",
      "Time taken: 26.94 seconds\n",
      "\n",
      "File: JDFirewall P4 1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Fortinet', 'Firewall']\n",
      "Time taken: 79.47 seconds\n",
      "\n",
      "File: JDFirewall P4v2.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Fortinet, Firewall.\n",
      "Time taken: 9.39 seconds\n",
      "\n",
      "File: JDFirewall P4v2.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Fortinet', 'Firewall']\n",
      "Time taken: 30.18 seconds\n",
      "\n",
      "File: Oracle DBA.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •8-10 years of experience in Oracle Instance maintenance and troubleshooting role\n",
      "•Good with standard DBA activities like installation, configuration and troubleshooting issues\n",
      "•Hardening of databases and implementing database best practices\n",
      "•Experience with Database refresh and clone activities\n",
      "•Strong Knowledge of Database Backup and recovery using RMAN and logical backups.\n",
      "•Ability to collaborate independently on critical issues with clients and provide root cause analysis\n",
      "•Worked with ADDM/ASH/AWR reports and performance tuning.\n",
      "•Good understanding of 3-tier application architecture\n",
      "•Good communication skills\n",
      "•Basic RedHat/Linux Administration\n",
      "•Basic scripting skills with Unix Shell Scripting, PowerShell, Python\n",
      "•Understanding of OEM and Oracle grid-control.\n",
      "•Knowledge of Oracle EBS, SQL Server and administration is a plus\n",
      "•Develop\n",
      "Time taken: 80.03 seconds\n",
      "\n",
      "File: Oracle DBA.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['8-10 years of experience in Oracle Instance maintenance and troubleshooting role', 'Good with standard DBA activities like installation, configuration and troubleshooting issues', 'Hardening of databases and implementing database best practices', 'Experience with Database Refresh and Clone activities', 'Strong Knowledge of Database Backup and recovery using RMAN and Logical Backups', 'Ability to collaborate independently on critical issues with clients and provide Root Cause Analysis', 'Worked with ADDM/ASH/AWR reports and performance tuning', 'Good communication skills', 'Basic Red Hat/Linux Administration', 'Basic scripting skills with Unix Shell Scripting, PowerShell, Python', 'Understanding of OEM and Oracle Grid-Control', 'Knowledge of Oracle EBS, SQL Server and Administration is a plus', 'Development background and knowledge of software development life cycle and tools (Jenkins, GitHub, Python)']\n",
      "Time taken: 62.99 seconds\n",
      "\n",
      "File: Oracle DBA_V1.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •Oracle Instance maintenance, \n",
      "•8-10 years of experience in Oracle Instance maintenance and troubleshooting role\n",
      "•Good with standard DBA activities like installation, configuration and troubleshooting issues\n",
      "•Hardening of databases and implementing database best practices\n",
      "•Experience with Database refresh and clone activities\n",
      "•Strong Knowledge of Database Backup and recovery using RMAN and logical backups.\n",
      "•Ability to collaborate independently on critical issues with clients and provide root cause analysis\n",
      "•Worked with ADDM/ASH/AWR reports and performance tuning.\n",
      "•Good understanding of 3-tier application architecture\n",
      "•Good communication skills\n",
      "•Desired skills:\n",
      "•Basic RedHat/Linux Administration\n",
      "•Basic scripting skills with Unix Shell Scripting, PowerShell, Python\n",
      "•Understanding of OEM and Oracle grid-control.\n",
      "•Knowledge of Oracle EBS, SQL Server and administration is\n",
      "Time taken: 82.89 seconds\n",
      "\n",
      "File: Oracle DBA_V1.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['Oracle Instance maintenance', '8-10 years of experience in Oracle Instance maintenance and troubleshooting role', 'Good with standard DBA activities like installation, configuration and troubleshooting issues', 'Hardening of databases and implementing database best practices', 'Experience with Database Refresh and Clone activities', 'Strong Knowledge of Database Backup and recovery using RMAN and Logical Backups', 'Ability to collaborate independently on critical issues with clients and provide Root Cause Analysis', 'Worked with ADDM/ASH/AWR reports and performance tuning', 'Good understanding of 3-tier application architecture', 'Good communication skills', 'Basic Red Hat/Linux Administration', 'Basic scripting skills with Unix Shell Scripting, PowerShell, Python', 'Understanding of OEM and Oracle Grid-Control', 'Knowledge of Oracle EBS, SQL Server and Administration is a plus']\n",
      "Time taken: 62.87 seconds\n",
      "\n",
      "File: Oracle DBA_V2.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •8-10 years of experience in Oracle Instance maintenance and troubleshooting role\n",
      "•Good with standard DBA activities like installation, configuration and troubleshooting issues\n",
      "•Hardening of databases and implementing database best practices\n",
      "•Experience with Database refresh and clone activities\n",
      "•Strong Knowledge of Database Backup and recovery using RMAN and logical backups.\n",
      "•Ability to collaborate independently on critical issues with clients and provide root cause analysis\n",
      "•Worked with one of ADDM, ASH, AWR reports\n",
      "•Performance tuning.\n",
      "•Good understanding of 3-tier application architecture\n",
      "•Good communication skills\n",
      "•Basic RedHat/Linux Administration\n",
      "•Basic scripting skills with Unix Shell Scripting, PowerShell, Python\n",
      "•Understanding of OEM and Oracle grid-control.\n",
      "•Knowledge of Oracle EBS, SQL Server and administration is\n",
      "Time taken: 112.69 seconds\n",
      "\n",
      "File: Oracle DBA_V2.docx\n",
      "Model: slim-summary-tool\n",
      "LLM Response: ['8-10 years of experience in Oracle Instance maintenance and troubleshooting role', 'Good with standard DBA activities like installation, configuration and troubleshooting issues', 'Hardening of databases and implementing database best practices', 'Experience with Database Refresh and Clone activities', 'Strong Knowledge of Database Backup and recovery using RMAN and Logical Backups', 'Ability to collaborate independently on critical issues with clients and provide Root Cause Analysis', 'Worked with ADDM, ASH, AWR reports', 'Performance tuning', 'Good communication skills', 'Basic Red Hat/Linux Administration', 'Basic scripting skills with Unix Shell Scripting, PowerShell, Python', 'Understanding of OEM and Oracle Grid-Control', 'Knowledge of Oracle EBS, SQL Server and Administration is a plus', 'Development background and knowledge of software development life cycle and tools (Jenkins, GitHub, Python)']\n",
      "Time taken: 200.77 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.models import ModelCatalog\n",
    "\n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing your .docx files\n",
    "input_dir = r'C:\\DataScience-TechERG\\LLM_JD\\Input_Files'\n",
    "\n",
    "# List of models to use\n",
    "models = [ \"llmware/bling-tiny-llama-v0\",\"slim-summary-tool\"]\n",
    "\n",
    "# Initialize the prompter\n",
    "prompter = Prompt()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.docx') and not file_name.startswith('~$'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # Read the document content\n",
    "        doc_content = read_docx(file_path)\n",
    "        \n",
    "        # Define the prompt\n",
    "        prompt = \"Extract Mandatory or Primary Skills or Skill Set\"\n",
    "        context = doc_content  # Using the content read from the .docx file as the context\n",
    "        \n",
    "        for model_name in models:\n",
    "            # Load the model from the catalog\n",
    "            prompter.load_model(model_name)\n",
    "            \n",
    "            # Record start time for operation\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate a response using the prompter\n",
    "            response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "            \n",
    "            # Calculate and print the time taken for the operation\n",
    "            time_taken = round(time.time() - start_time, 2)\n",
    "            \n",
    "            # Extract 'llm_response' from the response and print it\n",
    "            llm_response = response.get('llm_response', 'Response not found')\n",
    "            print(f\"File: {file_name}\")\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"LLM Response: {llm_response}\")\n",
    "            print(f\"Time taken: {time_taken} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > Loading Model: llmware/slim-summary-tiny...\n",
      "\n",
      " > Model llmware/slim-summary-tiny load time: 27.319137573242188 seconds\n",
      "File: Oracle DBA_V2.docx\n",
      "Model: llmware/slim-summary-tiny\n",
      "LLM Response: ['8-10 years of experience in Oracle Instance maintenance and troubleshooting role', 'Good with standard DBA activities like installation, configuration and troubleshooting issues', 'Hardening of databases and implementing database best practices', 'Experience with Database Backup and recovery using RMAN and logical backups', 'Ability to collaborate independently on critical issues with clients and provide root cause analysis', 'Worked with one of ADDM, ASH, AWR reports', 'Performance tuning', 'Good understanding of 3-tier application architecture', 'Good communication skills', 'Basic RedHat/Linux Administration', 'Basic scripting skills with Unix Shell Scripting, PowerShell, Python', 'Understanding of OEM and Oracle grid-control', 'Knowledge of Oracle EBS, SQL Server and administration is a plus', 'Development background and knowledge of software development life cycle and tools (Jenkins, GitHub, Python)']\n",
      "Time taken: 89.49 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.models import ModelCatalog\n",
    "import gzip\n",
    "import torch \n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def fast_start_prompting  (model_name):\n",
    "\n",
    "    \"\"\" This is the main example script - it loads the question list, loads the model and executes the prompts. \"\"\"\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # load in the 'hello world' test questions above\n",
    "    #test_list = hello_world_questions()\n",
    "\n",
    "    print(f\"\\n > Loading Model: {model_name}...\")\n",
    "\n",
    "    prompter = Prompt().load_model(model_name)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"\\n > Model {model_name} load time: {t1-t0} seconds\")\n",
    " \n",
    "    file_path = r'C:\\DataScience-TechERG\\LLM_JD\\Input_Files\\Oracle DBA_V2.docx'\n",
    "\n",
    "    if os.path.exists(file_path) and file_path.endswith('.docx'):\n",
    "    # Read the document content\n",
    "       doc_content = read_docx(file_path)\n",
    "    \n",
    "    # Define the prompt\n",
    "       prompt = \"Extract Mandatory or Primary Skills or Skill Set \"\n",
    "       context = doc_content\n",
    "\n",
    "       start_time = time.time()\n",
    "\n",
    "       response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "       time_taken = round(time.time() - start_time, 2)\n",
    "       \n",
    "       llm_response = response.get('llm_response', 'Response not found')\n",
    "       file_name = os.path.basename(file_path)\n",
    "       print(f\"File: {file_name}\")\n",
    "       print(f\"Model: {model_name}\")\n",
    "       print(f\"LLM Response: {llm_response}\")\n",
    "       print(f\"Time taken: {time_taken} seconds\\n\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"The file {file_path} does not exist or is not a .docx file.\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    llm_models = ModelCatalog().list_generative_models()\n",
    "\n",
    "    #   if you only want to see the local models\n",
    "    llm_local_models = ModelCatalog().list_generative_local_models()\n",
    "\n",
    "    #   to see only the open source models\n",
    "    llm_open_source_models = ModelCatalog().list_open_source_models()\n",
    "\n",
    "    #   we will print out the local models\n",
    "    #for i, models in enumerate(llm_local_models):\n",
    "        #print(\"models: \", i, models[\"model_name\"], models[\"model_family\"])\n",
    "\n",
    "    generative_models = [  \"llmware/bling-1b-0.1\",\n",
    "                           \"llmware/bling-tiny-llama-v0\", \n",
    "                           \"slim-summary-tool\",\n",
    "                           \"dragon-yi-answer-tool\",\n",
    "                           \"llmware/slim-extract\",\n",
    "                           \"llmware/bling-falcon-1b-0.1\",\n",
    "                           \"llmware/bling-cerebras-1.3b-0.1\",\n",
    "                           \"llmware/slim-summary-tiny\",\n",
    "                           \"llmware/slim-extract-tiny\",             \n",
    "                           \"llmware/slim-summary\",\n",
    "                           ]\n",
    "    \n",
    "\n",
    "    #   by default, we will select a gguf model requiring no additional imports\n",
    "    model_name = generative_models[7]\n",
    "\n",
    "    #   to swap in a gpt-4 openai model - uncomment these two lines\n",
    "    #   model_name = \"gpt-4\"\n",
    "    #   os.environ[\"USER_MANAGED_OPENAI_API_KEY\"] = \"<insert-your-openai-key>\"\n",
    "\n",
    "    fast_start_prompting(model_name)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmware.models import ModelCatalog\n",
    "from llmware.prompts import Prompt\n",
    "\n",
    "#   all models accessed through the ModelCatalog\n",
    "models = ModelCatalog().list_all_models()\n",
    "\n",
    "#   to use any model in the ModelCatalog - \"load_model\" method and pass the model_name parameter\n",
    "my_model = ModelCatalog().load_model(\"llmware/bling-phi-3-gguf\")\n",
    "output = my_model.inference(\"what is the future of AI?\", add_context=\"Here is the article to read\")\n",
    "\n",
    "#   to integrate model into a Prompt\n",
    "prompter = Prompt().load_model(\"llmware/bling-tiny-llama-v0\")\n",
    "response = prompter.prompt_main(\"what is the future of AI?\", context=\"Insert Sources of information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > Loading Model: llmware/bling-tiny-llama-v0...\n",
      "\n",
      " > Model llmware/bling-tiny-llama-v0 load time: 9.80860161781311 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Oracle DBA.docx\n",
      "Model: llmware/bling-tiny-llama-v0\n",
      "LLM Response: •8-10 years of experience in Oracle Instance maintenance and troubleshooting role\n",
      "•Good with standard DBA activities like installation, configuration and troubleshooting issues\n",
      "•Hardening of databases and implementing database best practices\n",
      "•Experience with Database refresh and clone activities\n",
      "•Strong Knowledge of Database Backup and recovery using RMAN and logical backups.\n",
      "•Ability to collaborate independently on critical issues with clients and provide root cause analysis\n",
      "•Worked with ADDM/ASH/AWR reports and performance tuning.\n",
      "•Good understanding of 3-tier application architecture\n",
      "•Good communication skills\n",
      "•Basic RedHat/Linux Administration\n",
      "•Basic scripting skills with Unix Shell Scripting, PowerShell, Python\n",
      "•Understanding of OEM and Oracle grid-control.\n",
      "•Knowledge of Oracle EBS, SQL Server and administration is a plus\n",
      "•Develop\n",
      "Time taken: 81.95 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.models import ModelCatalog\n",
    "import gzip\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to read text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def fast_start_prompting  (model_name):\n",
    "\n",
    "    \"\"\" This is the main example script - it loads the question list, loads the model and executes the prompts. \"\"\"\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # load in the 'hello world' test questions above\n",
    "    #test_list = hello_world_questions()\n",
    "\n",
    "    print(f\"\\n > Loading Model: {model_name}...\")\n",
    "\n",
    "    prompter = Prompt().load_model(model_name)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"\\n > Model {model_name} load time: {t1-t0} seconds\")\n",
    " \n",
    "    file_path = r'C:\\DataScience-TechERG\\LLM_JD\\Input_Files\\Oracle DBA.docx'\n",
    "\n",
    "    if os.path.exists(file_path) and file_path.endswith('.docx'):\n",
    "    # Read the document content\n",
    "       doc_content = read_docx(file_path)\n",
    "    \n",
    "    # Define the prompt\n",
    "       prompt = \"Extract Mandatory or Primary Skills or Skill Set \"\n",
    "       context = doc_content\n",
    "\n",
    "       start_time = time.time()\n",
    "\n",
    "       response = prompter.prompt_main(prompt, context=context, prompt_name=\"default_with_context\", temperature=0.3)\n",
    "\n",
    "       time_taken = round(time.time() - start_time, 2)\n",
    "       \n",
    "       llm_response = response.get('llm_response', 'Response not found')\n",
    "       file_name = os.path.basename(file_path)\n",
    "       print(f\"File: {file_name}\")\n",
    "       print(f\"Model: {model_name}\")\n",
    "       print(f\"LLM Response: {llm_response}\")\n",
    "       print(f\"Time taken: {time_taken} seconds\\n\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"The file {file_path} does not exist or is not a .docx file.\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    llm_models = ModelCatalog().list_generative_models()\n",
    "\n",
    "    #   if you only want to see the local models\n",
    "    llm_local_models = ModelCatalog().list_generative_local_models()\n",
    "\n",
    "    #   to see only the open source models\n",
    "    llm_open_source_models = ModelCatalog().list_open_source_models()\n",
    "\n",
    "    #   we will print out the local models\n",
    "    #for i, models in enumerate(llm_local_models):\n",
    "        #print(\"models: \", i, models[\"model_name\"], models[\"model_family\"])\n",
    "\n",
    "    generative_models = [  \"llmware/bling-1b-0.1\",\n",
    "                           \"llmware/bling-tiny-llama-v0\", \n",
    "                           \"slim-summary-tool\",\n",
    "                           \"dragon-yi-answer-tool\",\n",
    "                           \"llmware/slim-extract\",\n",
    "                           \"llmware/bling-falcon-1b-0.1\",\n",
    "                           \"llmware/bling-cerebras-1.3b-0.1\",\n",
    "                           \"llmware/slim-summary-tiny\",\n",
    "                           \"llmware/slim-extract-tiny\",             \n",
    "                           \"llmware/slim-summary\",\n",
    "                           ]\n",
    "    \n",
    "\n",
    "    #   by default, we will select a gguf model requiring no additional imports\n",
    "    model_name = generative_models[1]\n",
    "\n",
    "    #   to swap in a gpt-4 openai model - uncomment these two lines\n",
    "    #   model_name = \"gpt-4\"\n",
    "    #   os.environ[\"USER_MANAGED_OPENAI_API_KEY\"] = \"<insert-your-openai-key>\"\n",
    "\n",
    "    fast_start_prompting(model_name)\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
